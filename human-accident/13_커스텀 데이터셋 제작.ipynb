{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33310fc9-2465-421f-a55d-19453a1b28d4",
   "metadata": {},
   "source": [
    "# ğŸŸ© ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ì œì‘ ê³µë¶€í•˜ê¸°       \n",
    "\n",
    "https://tutorials.pytorch.kr/beginner/basics/data_tutorial.html  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786c77c2",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## ğŸŸ¢ **[Dataset ì´í•´í•˜ê¸°]** Dataset í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ì•„ì„œ ë‚´ê°€ ì§ì ‘ __len__ê³¼ __getitem__ì„ ì •ì˜  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64e90f48-8097-4443-823d-a408fbf1e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #í›ˆë ¨\n",
    "from torch.utils.data import Dataset, DataLoader #ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ì„ ë§Œë“¤ê¸° ìœ„í•¨\n",
    "\n",
    "import os #ë¡œì»¬ ì»´í“¨í„°ì—ì„œ ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´\n",
    "import cv2 # ì‹œê°í™” í•´ì„œ í‘œí˜„í•˜ê¸° ìœ„í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "519c53b9-7544-4e3e-88e6-bad912fa84f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ì•„ì„œ ìƒˆë¡œìš´ Datasetì„ ì •ì˜\n",
    "\n",
    "# __í•¨ìˆ˜__ : ë˜ë” í•¨ìˆ˜\n",
    "class SimpleDataset(Dataset):\n",
    "    # ì´ í´ë˜ìŠ¤ì˜ ê°ì²´ê°€ ìƒì„±ë˜ë©´ ë¬´ì¡°ê±´ __init__ í•¨ìˆ˜ê°€ ë™ì‘í•˜ê²Œ ë¨\n",
    "    # ê¸°ì´ˆ ë°ì´í„°, í•¨ìˆ˜ ë™ì‘í•˜ë„ë¡ ì—°ê²°\n",
    "    def __init__(self, t):\n",
    "        self.t = t\n",
    "\n",
    "    # ë°ì´í„°ì…‹ì˜ ê¸¸ì´ ë°˜í™˜\n",
    "    # íŒŒì´í† ì¹˜ê°€ í›ˆë ¨ì„ í•  ë•Œ len() -> ë°°ì¹˜ì‚¬ì´ì¦ˆ ëŒ€ë¹„ ì–¼ë§ˆë‚˜ ë°ì´í„°ì…‹ì´ ë‚¨ì•„ìˆëŠ” ì§€ ì²´í¬í•  ë•Œ ì”€\n",
    "    #    len(dataset) ì„ ì‹¤í–‰í•˜ë©´ ì´ í•¨ìˆ˜ê°€ í˜¸ì¶œë¨\n",
    "    def __len__(self):\n",
    "        return self.t\n",
    "\n",
    "    # ë°ì´í„° + ë¼ë²¨ -> í•œ ìŒì„ ë½‘ëŠ” ë° ì”€\n",
    "    # idx(index) = ëª‡ ë²ˆì§¸ ë°ì´í„°?\n",
    "    #    dataset[0] ì²˜ëŸ¼ íŠ¹ì • ë°ì´í„°ë¥¼ êº¼ë‚¼ ë•Œ ì‹¤í–‰ë¨\n",
    "    def __getitem__(self, idx):\n",
    "        # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ ì¸ë±ìŠ¤ ë²ˆí˜¸ë¥¼ LongTensor(ì •ìˆ˜í˜• í…ì„œ)ë¡œ ë°˜í™˜\n",
    "        return torch.LongTensor([idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a874b2b",
   "metadata": {},
   "source": [
    "Dataset í´ë˜ìŠ¤  \n",
    "   â”‚  \n",
    "   â”œâ”€â”€ __init__(ë°ì´í„° ì¤€ë¹„)  \n",
    "   â”‚  \n",
    "   â”œâ”€â”€ __len__() â†’ ë°ì´í„° ê°œìˆ˜ ì•Œë ¤ì¤Œ  \n",
    "   â”‚  \n",
    "   â””â”€â”€ __getitem__(i) â†’ ië²ˆì§¸ ë°ì´í„° êº¼ë‚´ì¤Œ  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "912551d8-bf89-4682-ad71-faa158c4409a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SimpleDataset í´ë˜ìŠ¤ë¥¼ ì´ìš©í•´ ì´ 7ê°œì˜ ë°ì´í„°ê°€ ìˆëŠ” ë°ì´í„°ì…‹ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "dataset = SimpleDataset(t=7)\n",
    "\n",
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "977684e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)  # 7 (ë°ì´í„° ê°œìˆ˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "807fcd3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ba6f2de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[3]  # tensor([3])  (ì„¸ ë²ˆì§¸ ë°ì´í„° êº¼ëƒ„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c294b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ†˜ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ ë¬´í•œëŒ€ë¡œ ë‚˜ì˜¤ëŠ” ì´ìœ ë¥¼ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.\n",
    "# for i, data in enumerate(dataset):\n",
    "#     print(f\"{i}ë²ˆì§¸ : {data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "279282d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97f97d6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## ğŸŸ¢ **[DataLoader ì´í•´í•˜ê¸°]** - ë°ì´í„°ì…‹ì„ í›ˆë ¨ì— ì‚¬ìš©í•˜ê¸° ì¢‹ê²Œ ë§Œë“¤ê¸°  \n",
    "\n",
    "ë§Œì•½ ë°ì´í„°ê°€ 100ë§Œ ê°œë¼ë©´, ì´ê±¸ í•œ ë²ˆì— ì²˜ë¦¬í•˜ê¸°ëŠ” ì–´ë µë‹¤. ê·¸ë˜ì„œ DataLoaderê°€ batch_sizeë§Œí¼ ë°ì´í„°ë¥¼ ì˜ë¼ì„œ ì¡°ê¸ˆì”© ëª¨ë¸ì— ì „ë‹¬í•´ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14371147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ ë°ì´í„°ì…‹ì„ í›ˆë ¨ì—ì„œ ì–´ë–»ê²Œ í™œìš©í•  ê²ƒì¸ê°€?\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,    # ì–´ë–¤ ë°ì´í„°ì…‹ì„ í›ˆë ¨ì— ì“¸ ê±°ë‹ˆ?\n",
    "    batch_size=2,       # ë°ì´í„°ì…‹ì„ ìª¼ê°œ ë°°ì¹˜ë¡œ ë§Œë“¦\n",
    "    shuffle=True,       # ë°ì´í„°ì…‹ì„ ì„ì–´ì£¼ëŠ”(ëœë¤)\n",
    "    drop_last=True,     # ë°°ì¹˜ì‚¬ì´ì¦ˆ ë§Œë“¤ê³  ë‚¨ì€ ë°ì´í„° ì“¸ê±°ì•¼? (True: ë²„ë¦¼)\n",
    "    # ì´ 7ê°œì˜ ë°ì´í„°ë¥¼ batch_size 2ê°œì”© ì§ì§€ì–´ì¤¬ì„ ë•Œ, ë‚˜ë¨¸ì§€ 1ê°œê°€ ë‚¨ì€ ê²ƒì„ ë²„ë¦½ë‹ˆë‹¤.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a7d6d02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x19ebad7d790>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f94d3967-dd95-49a3-b159-0db49c5b75ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0\n",
      "tensor([[4],\n",
      "        [2]])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "tensor([[0],\n",
      "        [6]])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "tensor([[5],\n",
      "        [3]])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "epoch : 1\n",
      "tensor([[0],\n",
      "        [4]])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "tensor([[3],\n",
      "        [6]])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "tensor([[2],\n",
      "        [5]])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "epoch : 2\n",
      "tensor([[1],\n",
      "        [4]])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "tensor([[3],\n",
      "        [0]])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "tensor([[6],\n",
      "        [5]])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataLoaderê°€ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ë°°ì¹˜(batch) ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì£¼ëŠ”ì§€ í•™ìŠµê³¼ì •ì„ ì´í•´í•˜ê¸° ì‰¬ìš´ ì˜ˆì‹œ\n",
    "\n",
    "# epoch: ì „ì²´ ë°ì´í„°ë¥¼ ëª¨ë‘ ì‚¬ìš©í•´ì„œ í•œ ë²ˆ í•™ìŠµí•˜ëŠ” ê³¼ì •\n",
    "\n",
    "for e in range(3):  # ì´ 3ë²ˆì˜ epochë™ì•ˆ í›ˆë ¨ì„ í•œë‹¤.\n",
    "    print(f\"epoch : {e}\")   # í˜„ì¬ ëª‡ë²ˆì§¸ epoch ì¸ì§€ ì¶œë ¥.\n",
    "\n",
    "    for batch in dataloader:    # dataset ì „ì²´ë¥¼ ëŒë©´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "        print(batch)\n",
    "        print(type(batch))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415fa8fb-192e-4a7b-837f-3c782be44030",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "## ğŸŸ¢ **[CustomData ë§Œë“¤ê¸°]** - (ì˜ˆì‹œ) ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì„ ì»¤ìŠ¤í…€í™” í•¨          \n",
    "- ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ ì´ë¯¸ì§€ íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ ì²˜ë¦¬í•˜ëŠ” ë‚˜ë§Œì˜ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ë§Œë“¤ê¸°!!!  \n",
    "\n",
    "- ì´ë¯¸ì§€(0~255) ìˆ«ìë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„° -> ë”¥ëŸ¬ë‹ -> í…ì„œ í˜•íƒœë¡œ í‘œí˜„  \n",
    "- í…ì„œ í˜•íƒœë¡œ ì´ë¯¸ì§€ë¥¼ í‘œí˜„í•´ ì¤˜ì•¼ í•¨.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "22f1c572-13ea-460a-9026-2028c71690dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision: PyTorchì—ì„œ ì´ë¯¸ì§€ ì²˜ë¦¬/ë°ì´í„°ì…‹ ê´€ë ¨ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "# ğŸ”¥transforms(ì „ì²˜ë¦¬): ì´ë¯¸ì§€ì˜ í¬ê¸°ë¥¼ ì¡°ì ˆí•˜ê±°ë‚˜, ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í…ì„œ(Tensor) í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ë“± (ìë¥´ê¸°, íšŒì „, í…ì„œ ë³€í™˜, ì •ê·œí™” ë“±)\n",
    "#                       ì´ë¯¸ì§€ ì „ì²˜ë¦¬(preprocessing) ê¸°ëŠ¥ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.\n",
    "from torchvision import transforms \n",
    "\n",
    "# PIL(Pillow) ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ Image: íŒŒì´ì¬ì—ì„œ ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ë‹¤ë£° ë•Œ ì‚¬ìš©í•˜ëŠ” ê¸°ë³¸ì ì¸ ë„êµ¬ì…ë‹ˆë‹¤.\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt # ì´ë¯¸ì§€ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d90c61-3e1f-4b0b-814d-2647f817c57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomData(Dataset):\n",
    "    def __init__(self, path, transform):\n",
    "        self.image_path = path  # ì „ì²´ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì €ì¥\n",
    "        # self.image_pathëŠ” ë³´í†µ ë¬¸ìì—´ ê²½ë¡œë“¤ì˜ ë¦¬ìŠ¤íŠ¸(ì˜ˆ: [\"data/a.jpg\",\"data/b.jpg\", ...])\n",
    "        # ë˜ëŠ” pathlib.Path ê°ì²´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤. \"path\"ë¼ëŠ” ì´ë¦„ ë•Œë¬¸ì— í´ë” ê²½ë¡œ í•˜ë‚˜ë¥¼ ë°›ì„ ê²ƒì²˜ëŸ¼ ë³´ì¼ ìˆ˜ ìˆìœ¼ë‹ˆ\n",
    "        # ì‹¤ì œë¡œëŠ” 'íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸'ì¸ì§€ 'í´ë” ê²½ë¡œ(ë‚´ë¶€ íƒìƒ‰ í•„ìš”)'ì¸ì§€ ëª…í™•íˆ í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "        \n",
    "        self.transform = transform  # ì´ë¯¸ì§€ì— ëŒ€í•´ ì¼ê´„ì ìœ¼ë¡œ ì ìš©í•  'ì „ì²˜ë¦¬'\n",
    "        # transformì€ callable (ì˜ˆ: torchvision.transforms.Compose([...]))ì´ì–´ì•¼ í•˜ë©°,\n",
    "        # ì…ë ¥ìœ¼ë¡œ PIL.Imageì„ ë°›ê³  ì¶œë ¥ìœ¼ë¡œ torch.Tensor(ë˜ëŠ” ë³€í™˜ëœ PIL/ndarray)ë¥¼ ë°˜í™˜í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.\n",
    "\n",
    "    def __len__(self):\n",
    "        # if (ì´ë¯¸ì§€ìˆ˜ == ë¼ë²¨ìˆ˜) return ì´ë¯¸ì§€ ìˆ˜\n",
    "        return len(self.image_path)\n",
    "        # __len__ì€ DataLoaderê°€ ì „ì²´ ë°ì´í„° ê°œìˆ˜ë¥¼ ì•Œê¸° ìœ„í•´ í˜¸ì¶œí•©ë‹ˆë‹¤.\n",
    "        # image_pathê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë¼ë©´(ì˜ˆ: glob ê²°ê³¼ê°€ ë¹ˆ ê²½ìš°) ì—¬ê¸°ì„œ ì—ëŸ¬ ë˜ëŠ” 0ì„ ë°˜í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "    # í…ì„œ í˜•íƒœë¡œ ë³€í™˜ëœ ì´ë¯¸ì§€ë¥¼ ëŒë ¤ì¤Œ\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.image_path[idx]\n",
    "        # image ë³€ìˆ˜ëŠ” 'íŒŒì¼ ê²½ë¡œ ë¬¸ìì—´'ì…ë‹ˆë‹¤. ì£¼ì„ê³¼ í˜¼ë™í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤(ì´ë¯¸ì§€ê°€ ì•„ë‹˜).\n",
    "        # ì˜ˆ: image == \"data/img_001.jpg\"\n",
    "\n",
    "        image = Image.open(image)  # í•´ë‹¹ ê²½ë¡œì—ì„œ ì´ë¯¸ì§€ë¥¼ ì½ì–´ ì˜´\n",
    "        # PIL.Image.openì€ íŒŒì¼ í•¸ë“¤ì„ ì—° ìƒíƒœë¡œ ë°˜í™˜(ì§€ì—° ë¡œë”©)í•©ë‹ˆë‹¤.\n",
    "        # íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ë‹«ìœ¼ë ¤ë©´ with Image.open(path) as img: ê°™ì€ ë¬¸ë²•ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n",
    "        # ë˜í•œ, ì±„ë„ í†µì¼ì„ ìœ„í•´ .convert('RGB') í˜¹ì€ .convert('L')ì„ í˜¸ì¶œí•˜ëŠ” ê²ƒì´ ì•ˆì „í•©ë‹ˆë‹¤.\n",
    "\n",
    "        if self.transform is not None:\n",
    "            result = self.transform(image)\n",
    "            # transformì´ ToTensor ë“±ì„ í¬í•¨í•˜ë©´ resultëŠ” torch.Tensor (C,H,W), dtype=float32, ê°’ë²”ìœ„ 0~1 ì´ ë©ë‹ˆë‹¤.\n",
    "            # ì£¼ì˜: transformì´ PIL ì´ë¯¸ì§€ë¥¼ ë³€ê²½í•˜ê³  ë‹«ì§€ ì•Šìœ¼ë¯€ë¡œ ì´ë¯¸ì§€ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ë¥¼ ì‹ ê²½ ì¨ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "        # csv,íŒŒì¼ ì´ë¦„ ë“±ì„ ì´ìš©í•´ì„œ ë°ì´í„°ì— ë§ëŠ” ë¼ë²¨ì„ ë°˜í™˜\n",
    "        label = 1\n",
    "        # í˜„ì¬ëŠ” í•˜ë“œì½”ë”©ëœ ë”ë¯¸ ë¼ë²¨(1)ì…ë‹ˆë‹¤. ì‹¤ì œ ì‘ì—…ì—ì„œëŠ”:\n",
    "        # - íŒŒì¼ëª…/í´ë” êµ¬ì¡°ì—ì„œ ë¼ë²¨ì„ íŒŒì‹±í•˜ê±°ë‚˜,\n",
    "        # - __init__ì— labels ë¦¬ìŠ¤íŠ¸/ë”•ì…”ë„ˆë¦¬ë¥¼ ì „ë‹¬í•˜ê±°ë‚˜,\n",
    "        # - CSV íŒŒì¼ì„ ì½ì–´ ë¼ë²¨ ë§µì„ ë§Œë“¤ì–´ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "        # ë¼ë²¨ì€ ì •ìˆ˜(int) ë˜ëŠ” torch.Tensor (ì˜ˆ: torch.tensor(label, dtype=torch.long)) ë¡œ ë°˜í™˜í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.\n",
    "\n",
    "        return result, label\n",
    "        # ì£¼ì˜: ë§Œì•½ self.transformì´ Noneì´ë©´ result ë³€ìˆ˜ê°€ ì •ì˜ë˜ì§€ ì•Šì•„ NameErrorê°€ ë‚©ë‹ˆë‹¤.\n",
    "        # ë”°ë¼ì„œ transform None ì¼€ì´ìŠ¤ë¥¼ ë°˜ë“œì‹œ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6effb542",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŸ¢ `CustomData`ì— ë„£ì„ `ì¸ìˆ˜` or `ë§¤ê°œë³€ìˆ˜` ì •ì˜í•˜ê¸°  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19134f7",
   "metadata": {},
   "source": [
    "\n",
    "### ğŸŸ¡ **[transforms.Compose]** - ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë°©ë²• ì •ì˜í•˜ê¸°  \n",
    "- ë§Œë“¤ì–´ë†“ì€ CustomData classë¥¼ ì‚¬ìš©í•˜ê¸° ì „ì— ì „ì²˜ë¦¬ ë°©ë²•ì„ ì •ì˜í•˜ì.  \n",
    "- ë”¥ëŸ¬ë‹ ëª¨ë¸ì— ì´ë¯¸ì§€ë¥¼ ë„£ê¸° ì „ì—, ëª¨ë“  ì´ë¯¸ì§€ì˜ í¬ê¸°ë¥¼ ê°™ê²Œ ë§Œë“¤ê³ , ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ê³„ì‚°í•˜ê¸° ì¢‹ì€ í˜•íƒœë¡œ ë°”ê¿”ì£¼ëŠ” ë“±ì˜ ì „ì²˜ë¦¬ ê³¼ì •ì´ í•„ìš”.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "81213a9a-3c33-4716-aa70-27dd0f6d7903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ëŸ¬ ê°€ì§€ ì „ì²˜ë¦¬ ë°©ë²•ì„ í•˜ë‚˜ë¡œ ë¬¶ì–´ì£¼ëŠ” transforms.Composeë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        # 1. ì´ë¯¸ì§€ í¬ê¸° ì¡°ì ˆ: ëª¨ë“  ì´ë¯¸ì§€ì˜ í¬ê¸°ë¥¼ 224x224 í”½ì…€ë¡œ ë§ì¶¥ë‹ˆë‹¤.\n",
    "        transforms.Resize((224, 224)),\n",
    "\n",
    "        # 2. í…ì„œ(Tensor)ë¡œ ë³€í™˜: ì´ë¯¸ì§€ë¥¼ ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” ìˆ«ì í–‰ë ¬(í…ì„œ)ë¡œ ë°”ê¿‰ë‹ˆë‹¤.\n",
    "        transforms.ToTensor(),\n",
    "\n",
    "        # 3. ì •ê·œí™”(Normalize): ì´ë¯¸ì§€ì˜ í”½ì…€ ê°’ ë²”ìœ„ë¥¼ ì¡°ì •í•˜ì—¬ ëª¨ë¸ì´ ë” ë¹ ë¥´ê³  ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•˜ë„ë¡ ë•ìŠµë‹ˆë‹¤.\n",
    "        #    ì•„ë˜ meanê³¼ std ê°’ì€ ImageNet ë°ì´í„°ì…‹ì—ì„œ ë¯¸ë¦¬ ê³„ì‚°ëœ ê°’ìœ¼ë¡œ, ë³´í†µ ê·¸ëŒ€ë¡œ ë§ì´ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3671a415",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ğŸŸ¡ **[path ì§€ì •]** - (ì˜ˆì‹œ) ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ë§Œë“¤ê¸°  (âŒ ë°ì´í„° ì—†ìŒ)  \n",
    "- ğŸ”¥ pathë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ì–´í•œë‹¤ëŠ” ê²ƒ  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b8d9c60d-9ea2-4cf6-9eb9-55d50f45e009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bump', 'fall-down', 'fall-off', 'hit', 'jam', 'no-accident']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin = \"./data/safety-data/human-accident\" # ì´ë¯¸ì§€ íŒŒì¼ì´ ëª¨ì—¬ìˆëŠ” directoryë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "\n",
    "os.listdir(origin)  # ğŸ”¥ originìœ¼ë¡œ ì •ì˜í•œ directoryì— ì¡´ì¬í•˜ëŠ” ê²ƒë“¤(ex. ì´ë¯¸ì§€ë“¤, ì˜ìƒë“¤, ë“±ë“±)ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1d5a136d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/safety-data/human-accident\\\\bump'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.listdir(origin)ë¡œ ë§Œë“  ë¦¬ìŠ¤íŠ¸ì˜ ìš”ì†Œ í•˜ë‚˜ì”©ì„ ê°€ì ¸ì™€ origin path ë’¤ì— ë¶™ì—¬ì„œ ë‹¤ì‹œ listë¡œ ë§Œë“­ë‹ˆë‹¤.\n",
    "image_list = [os.path.join(origin, x) for x in os.listdir(origin)]\n",
    "\n",
    "# ë§Œë“  image_listì—ì„œ ì²« ë²ˆì§¸ íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œë¥¼ í™•ì¸í•´ ë´…ë‹ˆë‹¤.\n",
    "image_list[0]    #  =  './data/safety-data/human-accident\\\\bump'   ...  windowsì—ì„œëŠ” '/' ì´ê±°ë§ê³ , '\\\\'ì´ê±¸ë¡œ ì¨ì•¼ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d710660b-a4ae-4f1e-89e7-d67fede046f4",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŸ¢ **[CustomData ì‚¬ìš©]** - (ì˜ˆì‹œ) ì´ë¯¸ì§€ë¦¬ìŠ¤íŠ¸ë¥¼ ë‚´ê°€ ë§Œë“  ë°ì´í„°ì…‹ í´ë˜ìŠ¤ì— ì ìš©  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344353a6-9a3a-4c16-93ad-9238543b867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ë¯¸ì§€ì— ëŒ€í•œ ê²½ë¡œ, ì¼ê´„ì ìœ¼ë¡œ ì ìš©í•  ì „ì²˜ë¦¬ ì´ 2ê°€ì§€ë¥¼ ì¸ìˆ˜ë¡œ ë„£ì–´ì¤Œ.\n",
    "dataset = CustomData(image_list, transform) \n",
    "# ğŸ”¥ğŸ”¥ğŸ”¥ ì–´ë–»ê²Œ ì‘ìš©í•˜ëŠ” ê²ƒì¼ê¹Œ?\n",
    "DATAS.DWWDWQD()\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,  # ğŸ”¥ğŸ”¥ğŸ”¥ ì–´ë–»ê²Œ ì‘ìš©í•˜ëŠ” ê²ƒì¼ê¹Œ?\n",
    "    batch_size=25,\n",
    "    shuffle=True,\n",
    "    drop_last=False,  # ë§ˆì§€ë§‰ì— ë‚¨ëŠ” ë°ì´í„°ë„ ì‚¬ìš©í•©ë‹ˆë‹¤. (False: ë²„ë¦¬ì§€ ì•ŠìŒ)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76ee5e9",
   "metadata": {},
   "source": [
    "### ğŸŸ¡ ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ DataLoaderì—ì„œ ì´ë¯¸ì§€ ë°ì´í„° ë°°ì¹˜ êº¼ë‚´ë³´ê¸°  \n",
    "- DataLoaderì—ì„œ ì´ë¯¸ì§€ ë°ì´í„°ê°€ ì–´ë–»ê²Œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë‚˜ì˜¤ëŠ”ì§€ í™•ì¸í•´ ë³´ê² ìŠµë‹ˆë‹¤.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "100ad6bc-d298-4b59-b1de-99f4ea6af71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x0000019EBAD965D0>\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: './data/safety-data/human-accident\\\\jam'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m dataiter = \u001b[38;5;28miter\u001b[39m(dataloader) \u001b[38;5;66;03m# iterate \u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(dataiter)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataiter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bb\\Desktop\\learn_deeplearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bb\\Desktop\\learn_deeplearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bb\\Desktop\\learn_deeplearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bb\\Desktop\\learn_deeplearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mCustomData.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     22\u001b[39m image = \u001b[38;5;28mself\u001b[39m.image_path[idx]\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# image ë³€ìˆ˜ëŠ” 'íŒŒì¼ ê²½ë¡œ ë¬¸ìì—´'ì…ë‹ˆë‹¤. ì£¼ì„ê³¼ í˜¼ë™í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤(ì´ë¯¸ì§€ê°€ ì•„ë‹˜).\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# ì˜ˆ: image == \"data/img_001.jpg\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m image = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# í•´ë‹¹ ê²½ë¡œì—ì„œ ì´ë¯¸ì§€ë¥¼ ì½ì–´ ì˜´\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# PIL.Image.openì€ íŒŒì¼ í•¸ë“¤ì„ ì—° ìƒíƒœë¡œ ë°˜í™˜(ì§€ì—° ë¡œë”©)í•©ë‹ˆë‹¤.\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ë‹«ìœ¼ë ¤ë©´ with Image.open(path) as img: ê°™ì€ ë¬¸ë²•ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# ë˜í•œ, ì±„ë„ í†µì¼ì„ ìœ„í•´ .convert('RGB') í˜¹ì€ .convert('L')ì„ í˜¸ì¶œí•˜ëŠ” ê²ƒì´ ì•ˆì „í•©ë‹ˆë‹¤.\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bb\\Desktop\\learn_deeplearning\\.venv\\Lib\\site-packages\\PIL\\Image.py:3513\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3512\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3513\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3514\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3515\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: './data/safety-data/human-accident\\\\jam'"
     ]
    }
   ],
   "source": [
    "dataiter = iter(dataloader) # iterate \n",
    "print(dataiter)\n",
    "\n",
    "batch = next(dataiter)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b703b224",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa4057-6894-44cf-a8b2-58a446b43e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°°ì¹˜ì—ëŠ” ì´ë¯¸ì§€ì™€ ë¼ë²¨ì´ í•¨ê»˜ ë“¤ì–´ìˆìœ¼ë¯€ë¡œ, ê°ê°ì„ imagesì™€ labels ë³€ìˆ˜ì— ì €ì¥.\n",
    "images, labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d22a0-819e-4731-a68f-5ac68804c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•œ ë°°ì¹˜ì— ë“¤ì–´ìˆëŠ” ì´ë¯¸ì§€ì™€ ë¼ë²¨ì˜ ê°œìˆ˜ë¥¼ í™•ì¸. (batch_sizeì™€ ë™ì¼)\n",
    "len(images), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7354c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlibë¡œ ì²« ë²ˆì§¸ ë°ì´í„° ë°°ì¹˜ ì¶œë ¥\n",
    "# ì´ë¯¸ì§€ê°€ ë°°ì¹˜ (N, C, H, W) í˜•íƒœë¼ê³  ê°€ì • (ì˜ˆ: N=25, C=3, H=224, W=224)\n",
    "def imshow(img, title=None):\n",
    "    \"\"\"ì´ë¯¸ì§€ ì‹œê°í™”ë¥¼ ìœ„í•œ í•¨ìˆ˜\"\"\"\n",
    "    img = img.permute(1, 2, 0)  # (C, H, W) -> (H, W, C)\n",
    "    plt.imshow(img)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "\n",
    "# ë°°ì¹˜ì˜ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ì¶œë ¥\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(len(images)):\n",
    "    plt.subplot(5, 5, i + 1)  # 5x5 ê·¸ë¦¬ë“œ ìƒì„± (25ê°œì˜ ì´ë¯¸ì§€)\n",
    "    imshow(images[i])\n",
    "    plt.title(f\"Label: {labels[i].item()}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefb5877",
   "metadata": {},
   "source": [
    "ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´, DataLoaderê°€ 25ê°œì˜ ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ë°ì´í„°(í…ì„œ í˜•íƒœ)ì™€ 25ê°œì˜ ë¼ë²¨ì„ í•˜ë‚˜ì˜ batchë¡œ ë¬¶ì–´ì„œ ë°˜í™˜í•´ì£¼ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "ì´ì œ ì´ batch ë°ì´í„°ë¥¼ ë”¥ëŸ¬ë‹ ëª¨ë¸ì— ë„£ì–´ì„œ í•™ìŠµì„ ì§„í–‰í•˜ë©´ ë©ë‹ˆë‹¤.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87a253",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "\n",
    "# ğŸŸ© W & B (Weight & Biases)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee69ca70-2db5-4458-ab62-c0121c211818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f92f6d-7c12-4ff1-a0ad-19e577a0f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cff393-4dbc-4552-8183-f2e132d7f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"poth_resnet2\",  # í”„ë¡œì íŠ¸ ì´ë¦„\n",
    "    #config.yaml\n",
    "    config={\n",
    "        \"epochs\": 5,\n",
    "        \"batch_size\": 25,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"model\": \"ResNet18\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b237961-98f7-403d-8bb4-14de61def7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet ëª¨ë¸ ì •ì˜\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_classes = 2  # ë°ì´í„°ì…‹ì˜ í´ë˜ìŠ¤ ê°œìˆ˜ì— ë§ê²Œ ì„¤ì •\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# GPU ì‚¬ìš© ì—¬ë¶€\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# ğŸ”¥ ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ëª¨ë¸ í›ˆë ¨\n",
    "num_epochs = wandb.config.epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)  # ë°ì´í„° GPUë¡œ ì´ë™\n",
    "\n",
    "        optimizer.zero_grad()  # ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”\n",
    "        outputs = model(images)  # ìˆœì „íŒŒ\n",
    "        loss = criterion(outputs, labels)  # ì†ì‹¤ ê³„ì‚°\n",
    "        loss.backward()  # ì—­ì „íŒŒ\n",
    "        optimizer.step()  # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # **WandB ë°°ì¹˜ ë¡œê¹…**\n",
    "        wandb.log({\"batch_loss\": loss.item()})\n",
    "\n",
    "    # ì—í¬í¬ ì¢…ë£Œ ì‹œ í‰ê·  ì†ì‹¤ ë¡œê·¸ ê¸°ë¡\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    wandb.log({\"epoch_loss\": avg_loss, \"epoch\": epoch + 1})\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥ (ì„ íƒ ì‚¬í•­)\n",
    "torch.save(model.state_dict(), \"resnet18_poth_dataset.pth\")\n",
    "wandb.save(\"resnet18_poth_dataset.pth\")\n",
    "print(\"Model saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
