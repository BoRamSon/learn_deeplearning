{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a4e0c29",
   "metadata": {},
   "source": [
    "# 🟩 데이터 전처리  \n",
    "\n",
    "- 모델 선택 (나는 이 모델을 사용하여 학습 및 구현을 할 것이다.)  \n",
    "    - cnn-lstm  \n",
    "        - https://github.com/pranoyr/cnn-lstm  \n",
    "        - Action (Video) Classification  \n",
    "\n",
    "- 해당 모델의 데이터셋이 도대체 어떻게 생긴 것인지 먼저 파악해야함.  \n",
    "    - https://github.com/pranoyr/cnn-lstm/blob/master/dataset.py  \n",
    "        - **`UCF101`**  \n",
    "            ```python\n",
    "            def get_training_set(opt, spatial_transform, temporal_transform,  \n",
    "                    target_transform):  \n",
    "                if opt.dataset == 'ucf101':  \n",
    "                    training_data = UCF101(  \n",
    "                        opt.video_path,  \n",
    "                        opt.annotation_path,  \n",
    "                        'training',  \n",
    "                        spatial_transform=spatial_transform,  \n",
    "                        temporal_transform=temporal_transform,  \n",
    "                        target_transform=target_transform)  \n",
    "\n",
    "                return training_data  \n",
    "            ```\n",
    "\n",
    "- UCF101을 사용함.  \n",
    "    - https://www.kaggle.com/datasets/matthewjansen/ucf101-action-recognition  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f407b8",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## 🟢 1. 파악하기  \n",
    "\n",
    "### 🟡 UCF101이라는게 무엇인가?  \n",
    "https://wikidocs.net/164441  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 🟡 UCF101기준으로 전처리해야한다.  \n",
    "- 조건  \n",
    "    - 먼저 video를 기준으로 하며 그에 따르는 lable (annotation???)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f15d387",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## 🟢 2. 먼저 동영상과 라벨을 가져와서 가공하여 저장 (전처리)  \n",
    "(일단 먼저 하나의 동영상만 가져와서 해보겠습니다.)  \n",
    "\n",
    "https://bkshin.tistory.com/entry/OpenCV-3-%EC%9D%B4%EB%AF%B8%EC%A7%80-%EC%9E%85%EC%B6%9C%EB%A0%A5  \n",
    "https://blog.naver.com/life4happy/222499735708  \n",
    "https://scribblinganything.tistory.com/492  \n",
    "https://yonghyn.tistory.com/87  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0641b40",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 🟡 2-1. 비디오 파일을 녹화, 저장하는 절차에 대해서 알아보겠습니다.  \n",
    "\n",
    "- 동영상 파일 읽기  \n",
    "\n",
    "- 주요 함수  \n",
    "    | 함수/변수 | 설명 |  \n",
    "    | :--- | :--- |\n",
    "    | `cv2.VideoCapture()` | 현재 입력된 동영상 파일의 정보를 불러옵니다. 동영상 파일을 읽기 위해 필요한 프레임 설정, 코덱 등의 정보를 가져옵니다. |  \n",
    "    | `capture.read()` | 동영상 파일에서 프레임을 가져와 압축을 해제한 후, `ret`(bool)과 `frame`(ndarray)을 반환합니다. `ret`는 프레임을 정상적으로 읽어왔는지 알려주고, `frame`에는 현재 프레임 정보가 담깁니다. |  \n",
    "    | `cv2.CAP_PROP_POS_FRAMES` | 현재 프레임의 수 |  \n",
    "    | `cv2.CAP_PROP_FRAME_COUNT` | 총 프레임의 수 |  \n",
    "    | `cv2.imshow(videoName, frame)` | `videoName`은 윈도우에 표시할 이름이고, `frame`은 윈도우에 출력할 동영상 프레임 변수입니다. |  \n",
    "    | `cv2.waitKey(33) == ord('q')` | `waitKey`는 입력된 ms만큼 대기한 후 다음 프레임으로 넘어가는 대기 변수입니다. `ord`는 문자를 유니코드 값으로 변환하는 데 사용됩니다. 이 코드는 다음 프레임으로 넘어가기 위해 대기하는 동안 'q'가 입력되었는지 확인합니다. |  \n",
    "    | `capture.release()` | 동영상 파일을 닫고 메모리를 해제합니다. |  \n",
    "    | `cv2.destroyAllWindows()` | 동영상 재생이 끝난 후 모든 윈도우를 닫습니다. |  \n",
    "\n",
    "<br>\n",
    "\n",
    "- VideoWriter객체를 생성, 인코딩 코덱,초당 프레임, 프레임 크기 등의 환경을 기반으로, 이미지 프레임을 write()함수를 이용하여, 비디오파일에 저장/녹화하게 됩니다.  \n",
    "\n",
    "- 주요 함수  \n",
    "    | 함수 | 설명 | 반환값 |  \n",
    "    | :--- | :--- | :--- |\n",
    "    | `cv2.VideoWriter([filename, fourcc, fps, frameSize[, isColor]])` | 비디오 출력 객체를 생성합니다.<br>• **fourcc**: 비디오 저장 방식/파일명을 나타내는 코덱 문자입니다.<br>• **fps**: 초당 프레임<br>• **frameSize**: 동영상 화면 사이즈 | `VideoWriter object` |  \n",
    "    | `cv2.VideoWriter.write(image)` | 비디오 파일에 이미지 프레임을 출력, 저장합니다. | - |  \n",
    "    | `cv2.VideoWriter.release()` | 비디오 출력 객체를 해제합니다. | - |  \n",
    "    | `cv2.VideoWriter.get(id)` | 동영상 파일의 특성값(property)을 확인합니다. | - |  \n",
    "\n",
    "<br>\n",
    "\n",
    "- VideoWirter에 사용되는 인코딩 방법은 정의를 해줘야 합니다. `fourcc`에 지정할 수 있는 인코딩 방법입니다.  \n",
    "\n",
    "- `avi`, `mp4`는 `XVID`로 지정하면 됩니다.  \n",
    "\n",
    "- 다음은 `fourcc`로 지정할 수 있는 **`코덱의 종류`** 입니다.  \n",
    "    | fourcc | 코덱명 |  \n",
    "    | :--- | :--- |\n",
    "    | `cv2.VideoWriter_fourcc(*'PIM1')` | MPEG-1 |  \n",
    "    | `cv2.VideoWriter_fourcc(*'MJPG')` | Motion-JPEG |  \n",
    "    | `cv2.VideoWriter_fourcc(*'DIVX')` | DIVX 4.0 이후 버젼 |  \n",
    "    | `cv2.VideoWriter_fourcc(*'XVID')` | XVID, MPEG-4 |  \n",
    "    | `cv2.VideoWriter_fourcc(*'MPEG')` | MPEG |  \n",
    "    | `cv2.VideoWriter_fourcc(*'X264')` | H.264/AVC |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbc0bb8",
   "metadata": {},
   "source": [
    "#### ⚫ 동영상 경로 확인 및 지정  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba1fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# load .env\n",
    "load_dotenv()\n",
    "\n",
    "# [MacOS] 현재 데이터셋이 있는 경로\n",
    "mac_path = os.environ.get(\"macbook_path\")\n",
    "\n",
    "# [Windows] 현재 데이터셋이 있는 경로\n",
    "windows_source_path = \"D:\\\\078.스마트 제조 시설 안전 감시를 위한 데이터\\\\3.개방데이터\\\\1.데이터\\\\Validation\\\\01.원천데이터\\\\VS\\\\human-accident\"\n",
    "windows_lableing_path = \"D:\\\\078.스마트 제조 시설 안전 감시를 위한 데이터\\\\3.개방데이터\\\\1.데이터\\\\Validation\\\\02.라벨링데이터\\\\VL\\\\human-accident\"\n",
    "# 반드시 마지막 최종 경로는 폴더가 아닌 파일이 되어야 한다. 그렇지 않으면 PermissionError가 발생한다.\n",
    "\n",
    "# 내가 전처리할 경로\n",
    "data_source_path = \"./data/safety-data/Validation/01_source_data/human-accident/\"  # 영상 자체가 라벨이 되는 것이다.\n",
    "# data_labeling_path = \"./data/safety-data/Validation/02_labeling_data/human-accident/\"   # 라벨데이터는 필요없습니다.\n",
    "\n",
    "# human-accident의 class\n",
    "human_accident_class = [\"bump\", \"fall-down\", \"fall-off\", \"hit\", \"jam\", \"no-accident\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ed692163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\078.스마트 제조 시설 안전 감시를 위한 데이터\\3.개방데이터\\1.데이터\\Validation\\01.원천데이터\\VS\\human-accident\\bump\\rgb\\human-accident_bump_rgb_0001_cctv1\\human-accident_bump_rgb_0001_cctv1.mp4\n"
     ]
    }
   ],
   "source": [
    "# 임시로 특정 경로를 고정시켜둠.\n",
    "# 하나의 동영상을 타겟으로 path를 고정시켰습니다.\n",
    "fix_path = (\n",
    "    \"\\\\\"\n",
    "    + \"rgb\"\n",
    "    + \"\\\\\"\n",
    "    + \"human-accident_bump_rgb_0001_cctv1\"\n",
    "    + \"\\\\\"\n",
    "    + \"human-accident_bump_rgb_0001_cctv1.mp4\"\n",
    ")\n",
    "\n",
    "decided_path = windows_source_path + \"\\\\\" + human_accident_class[0] + fix_path\n",
    "print(decided_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e939ee",
   "metadata": {},
   "source": [
    "#### ⚫ 동영상 객체 생성  \n",
    "cv2.VideoCapture() : 동영상 입력하는 OpenCV의 함수이다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d1185cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# 동영상 객체 생성\n",
    "cap = cv2.VideoCapture(decided_path)  # 동영상 캡쳐 객체 생성  ---①"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd4f1f",
   "metadata": {},
   "source": [
    "#### ⚫ 동영상 파일 단순 읽기  \n",
    "- FRAME  \n",
    "    - 동영상은 여러 프레임 이미지의 합이라고 생각하시면 위 코드를 이해하기가 쉽습니다. 단편적인 프레임 이미지를 빠른 시간에 보여주면 움직이는 동영상처럼 보이죠? 위 코드도 하나의 프레임 이미지를 빠른 시간에 보여주는 방식으로 동작합니다. cv2.waitKey(25) 코드가 없다면 프레임이 너무 빨리 넘어가 눈으로 볼 수 없습니다. 하지만 25ms의 지연을 주면서 각 프레임을 화면에 표시해주기 때문에 우리는 동영상을 볼 수 있습니다. cv2.waitKey() 안에 들어가는 숫자를 지연 시간이라고 합니다. 25보다 작은 숫자를 넣어주면 화면이 동영상이 빠르게 재생되는 것처럼 보이고, 25보다 큰 숫자를 넣어주면 동영상이 천천히 재생되는 것처럼 보일 것입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "16a94fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fps : 30\n",
      "fps_total : 393\n",
      "width : 1920\n",
      "height : 1080\n",
      "size : (1920, 1080)\n"
     ]
    }
   ],
   "source": [
    "# 동영상의 fps를 확인\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "print(f\"fps : {fps}\")\n",
    "\n",
    "# 총 프레임의 수\n",
    "fps_total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(f\"fps_total : {fps_total}\")\n",
    "\n",
    "# 동영상의 넓이 확인\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "print(f\"width : {width}\")\n",
    "\n",
    "# 동영상의 높이 확인\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(f\"height : {height}\")\n",
    "\n",
    "# width, height\n",
    "size = (width, height)\n",
    "print(f\"size : {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c8c68ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 동영상 파일 단순 읽어서 확인합니다!!!!\n",
    "\n",
    "# if cap.isOpened():                      # 캡쳐 객체 초기화 확인 (동영상 파일 열기 성공 여부 확인)\n",
    "#     while True:\n",
    "#         ret, img = cap.read()           # 동영상 파일에서 프레임 읽기\n",
    "#         if ret:  # 프레임 읽기 정상\n",
    "#             cv2.imshow(decided_path, img)  # (제목, 표시하고자하는 img)를 지정하여 화면에 표시\n",
    "#             cv2.waitKey(25)             # 25ms 지연(40fps로 가정)\n",
    "#             print(\n",
    "#                 f\"현재 프레임 확인 : {int(cap.get(cv2.CAP_PROP_POS_FRAMES))}\"\n",
    "#             )  # 현재 프레임 확인\n",
    "#         else:                           # 다음 프레임 읽을 수 없음..\n",
    "#             break                       # 재생 완료.. (while문 종료)\n",
    "# else:\n",
    "#     print(\"can't open video.\")          # 캡쳐 객체 초기화 실패\n",
    "# cap.release()                           # 캡쳐 자원 반납 (메모리 반납)\n",
    "# cv2.destroyAllWindows()                 # 재생이 끝난 후 imshow 창 모두 닫기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c45a97",
   "metadata": {},
   "source": [
    "#### ⚫ 동영상 쓰기 (인코딩)  \n",
    "- 읽어들인 영상 프레임들을 저장하는 방식입니다.  \n",
    "- 기존 동영상 파일을 읽는 것뿐만 아니라,  \n",
    "- cv2.VideoWriter  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b7d0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정상적으로 재생 후 종료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "four_cc = cv2.VideoWriter_fourcc(*\"DIVX\")\n",
    "\n",
    "# DIVX 인코딩 방법 정의\n",
    "out_path_and_setting = cv2.VideoWriter(\n",
    "    # data_source_path + human_accident_class[0] + \"/test_out.avi\",  # 저장할 PATH\n",
    "    \"./data/test_out.mp4\",\n",
    "    four_cc,  # four_cc : fourcc 코덱 선정\n",
    "    fps,  # fps : 프레임 수\n",
    "    size,  # size : (튜플) 화면의 가로x세로 값\n",
    "    isColor=False,\n",
    ")\n",
    "\n",
    "if cap.isOpened():  # 캡쳐 객체 초기화 확인 (동영상 파일 열기 성공 여부 확인)\n",
    "    while True:\n",
    "        ret, img = cap.read()  # 동영상 파일에서 프레임 읽기\n",
    "\n",
    "        # 프레임 읽기 정상\n",
    "        if ret:\n",
    "            # 🔥 새로 쓰기\n",
    "            out_path_and_setting.write(img)\n",
    "\n",
    "            # imshow(제목, 표시하고자하는 img) 를 지정하여 화면에 표시\n",
    "            cv2.imshow(decided_path, img)\n",
    "            cv2.waitKey(25)  # 25ms 지연(40fps로 가정)\n",
    "\n",
    "            # print(f\"현재 프레임 확인 : {int(cap.get(cv2.CAP_PROP_POS_FRAMES))}\")\n",
    "\n",
    "        else:  # 다음 프레임 읽을 수 없음..\n",
    "            print(f\"정상적으로 재생 후 종료되었습니다.\")\n",
    "            break  # 재생 완료.. (while문 종료)\n",
    "\n",
    "else:\n",
    "    print(\"can't open video.\")  # 캡쳐 객체 초기화 실패\n",
    "\n",
    "cap.release()  # 캡쳐 자원 반납 (메모리 반납)\n",
    "out_path_and_setting.release()\n",
    "cv2.destroyAllWindows()  # 재생이 끝난 후 imshow 창 모두 닫기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced2091f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 🟡 1-.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58b3e58",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 🟡 1-.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db83b61d",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## 🟢 3.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e987b7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 🟡 4-.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c74ef30",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 🟡 4-.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580ee46e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 🟡 4-.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30acebf",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 🟡 4-.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6d299f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 🟡 4-.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1549140c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 🟡 4-.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3515bce8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 🟡 4-.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
