{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33310fc9-2465-421f-a55d-19453a1b28d4",
   "metadata": {},
   "source": [
    "# 🟩 커스텀 데이터셋 제작 공부하기       \n",
    "\n",
    "https://tutorials.pytorch.kr/beginner/basics/data_tutorial.html  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786c77c2",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 🟢 **[Dataset 이해하기]** Dataset 클래스를 상속받아서 내가 직접 __len__과 __getitem__을 정의  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64e90f48-8097-4443-823d-a408fbf1e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #훈련\n",
    "from torch.utils.data import Dataset, DataLoader #커스텀 데이터셋을 만들기 위함\n",
    "\n",
    "import os #로컬 컴퓨터에서 데이터셋을 불러오기 위해\n",
    "import cv2 # 시각화 해서 표현하기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "519c53b9-7544-4e3e-88e6-bad912fa84f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 클래스를 상속받아서 새로운 Dataset을 정의\n",
    "\n",
    "# __함수__ : 던더 함수\n",
    "class SimpleDataset(Dataset):\n",
    "    # 이 클래스의 객체가 생성되면 무조건 __init__ 함수가 동작하게 됨\n",
    "    # 기초 데이터, 함수 동작하도록 연결\n",
    "    def __init__(self, t):\n",
    "        self.t = t\n",
    "\n",
    "    # 데이터셋의 길이 반환\n",
    "    # 파이토치가 훈련을 할 때 len() -> 배치사이즈 대비 얼마나 데이터셋이 남아있는 지 체크할 때 씀\n",
    "    #    len(dataset) 을 실행하면 이 함수가 호출됨\n",
    "    def __len__(self):\n",
    "        return self.t\n",
    "\n",
    "    # 데이터 + 라벨 -> 한 쌍을 뽑는 데 씀\n",
    "    # idx(index) = 몇 번째 데이터?\n",
    "    #    dataset[0] 처럼 특정 데이터를 꺼낼 때 실행됨\n",
    "    def __getitem__(self, idx):\n",
    "        # 여기서는 단순히 인덱스 번호를 LongTensor(정수형 텐서)로 반환\n",
    "        return torch.LongTensor([idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a874b2b",
   "metadata": {},
   "source": [
    "Dataset 클래스  \n",
    "   │  \n",
    "   ├── __init__(데이터 준비)  \n",
    "   │  \n",
    "   ├── __len__() → 데이터 개수 알려줌  \n",
    "   │  \n",
    "   └── __getitem__(i) → i번째 데이터 꺼내줌  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "912551d8-bf89-4682-ad71-faa158c4409a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SimpleDataset 클래스를 이용해 총 7개의 데이터가 있는 데이터셋을 만듭니다.\n",
    "dataset = SimpleDataset(t=7)\n",
    "\n",
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "977684e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)  # 7 (데이터 개수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "807fcd3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ba6f2de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[3]  # tensor([3])  (세 번째 데이터 꺼냄)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c294b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🆘🔥🔥🔥🔥🔥 무한대로 나오는 이유를 모르겠습니다.\n",
    "# for i, data in enumerate(dataset):\n",
    "#     print(f\"{i}번째 : {data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "279282d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97f97d6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 🟢 **[DataLoader 이해하기]** - 데이터셋을 훈련에 사용하기 좋게 만들기  \n",
    "\n",
    "만약 데이터가 100만 개라면, 이걸 한 번에 처리하기는 어렵다. 그래서 DataLoader가 batch_size만큼 데이터를 잘라서 조금씩 모델에 전달해주는 역할을 합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14371147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 데이터셋을 훈련에서 어떻게 활용할 것인가?\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,    # 어떤 데이터셋을 훈련에 쓸 거니?\n",
    "    batch_size=2,       # 데이터셋을 쪼개 배치로 만듦\n",
    "    shuffle=True,       # 데이터셋을 섞어주는(랜덤)\n",
    "    drop_last=True,     # 배치사이즈 만들고 남은 데이터 쓸거야? (True: 버림)\n",
    "    # 총 7개의 데이터를 batch_size 2개씩 짝지어줬을 때, 나머지 1개가 남은 것을 버립니다.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a7d6d02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x19ebad7d790>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f94d3967-dd95-49a3-b159-0db49c5b75ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0\n",
      "tensor([[4],\n",
      "        [2]])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "tensor([[0],\n",
      "        [6]])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "tensor([[5],\n",
      "        [3]])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "epoch : 1\n",
      "tensor([[0],\n",
      "        [4]])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "tensor([[3],\n",
      "        [6]])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "tensor([[2],\n",
      "        [5]])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "epoch : 2\n",
      "tensor([[1],\n",
      "        [4]])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "tensor([[3],\n",
      "        [0]])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "tensor([[6],\n",
      "        [5]])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataLoader가 데이터를 어떻게 배치(batch) 단위로 나누어 주는지 학습과정을 이해하기 쉬운 예시\n",
    "\n",
    "# epoch: 전체 데이터를 모두 사용해서 한 번 학습하는 과정\n",
    "\n",
    "for e in range(3):  # 총 3번의 epoch동안 훈련을 한다.\n",
    "    print(f\"epoch : {e}\")   # 현재 몇번째 epoch 인지 출력.\n",
    "\n",
    "    for batch in dataloader:    # dataset 전체를 돌면서 학습합니다.\n",
    "        print(batch)\n",
    "        print(type(batch))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415fa8fb-192e-4a7b-837f-3c782be44030",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "## 🟢 **[CustomData 만들기]** - (예시) 이미지 데이터셋을 커스텀화 함          \n",
    "- 이제 본격적으로 이미지 파일을 불러와서 처리하는 나만의 데이터셋 클래스 만들기!!!  \n",
    "\n",
    "- 이미지(0~255) 숫자로 이루어진 데이터 -> 딥러닝 -> 텐서 형태로 표현  \n",
    "- 텐서 형태로 이미지를 표현해 줘야 함.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "22f1c572-13ea-460a-9026-2028c71690dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision: PyTorch에서 이미지 처리/데이터셋 관련 기능을 제공하는 라이브러리\n",
    "# 🔥transforms(전처리): 이미지의 크기를 조절하거나, 딥러닝 모델이 이해할 수 있는 텐서(Tensor) 형태로 변환하는 등 (자르기, 회전, 텐서 변환, 정규화 등)\n",
    "#                       이미지 전처리(preprocessing) 기능을 담당합니다.\n",
    "from torchvision import transforms \n",
    "\n",
    "# PIL(Pillow) 라이브러리의 Image: 파이썬에서 이미지를 불러오고 다룰 때 사용하는 기본적인 도구입니다.\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt # 이미지 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d90c61-3e1f-4b0b-814d-2647f817c57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomData(Dataset):\n",
    "    def __init__(self, path, transform):\n",
    "        self.image_path = path  # 전체 이미지 경로를 저장\n",
    "        # self.image_path는 보통 문자열 경로들의 리스트(예: [\"data/a.jpg\",\"data/b.jpg\", ...])\n",
    "        # 또는 pathlib.Path 객체들의 리스트로 기대됩니다. \"path\"라는 이름 때문에 폴더 경로 하나를 받을 것처럼 보일 수 있으니\n",
    "        # 실제로는 '파일 경로 리스트'인지 '폴더 경로(내부 탐색 필요)'인지 명확히 해야 합니다.\n",
    "        \n",
    "        self.transform = transform  # 이미지에 대해 일괄적으로 적용할 '전처리'\n",
    "        # transform은 callable (예: torchvision.transforms.Compose([...]))이어야 하며,\n",
    "        # 입력으로 PIL.Image을 받고 출력으로 torch.Tensor(또는 변환된 PIL/ndarray)를 반환하는 것이 일반적입니다.\n",
    "\n",
    "    def __len__(self):\n",
    "        # if (이미지수 == 라벨수) return 이미지 수\n",
    "        return len(self.image_path)\n",
    "        # __len__은 DataLoader가 전체 데이터 개수를 알기 위해 호출합니다.\n",
    "        # image_path가 리스트가 아니라면(예: glob 결과가 빈 경우) 여기서 에러 또는 0을 반환할 수 있습니다.\n",
    "\n",
    "\n",
    "    # 텐서 형태로 변환된 이미지를 돌려줌\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.image_path[idx]\n",
    "        # image 변수는 '파일 경로 문자열'입니다. 주석과 혼동하면 안 됩니다(이미지가 아님).\n",
    "        # 예: image == \"data/img_001.jpg\"\n",
    "\n",
    "        image = Image.open(image)  # 해당 경로에서 이미지를 읽어 옴\n",
    "        # PIL.Image.open은 파일 핸들을 연 상태로 반환(지연 로딩)합니다.\n",
    "        # 파일을 안전하게 닫으려면 with Image.open(path) as img: 같은 문법을 권장합니다.\n",
    "        # 또한, 채널 통일을 위해 .convert('RGB') 혹은 .convert('L')을 호출하는 것이 안전합니다.\n",
    "\n",
    "        if self.transform is not None:\n",
    "            result = self.transform(image)\n",
    "            # transform이 ToTensor 등을 포함하면 result는 torch.Tensor (C,H,W), dtype=float32, 값범위 0~1 이 됩니다.\n",
    "            # 주의: transform이 PIL 이미지를 변경하고 닫지 않으므로 이미지 리소스 관리를 신경 써야 합니다.\n",
    "\n",
    "        # csv,파일 이름 등을 이용해서 데이터에 맞는 라벨을 반환\n",
    "        label = 1\n",
    "        # 현재는 하드코딩된 더미 라벨(1)입니다. 실제 작업에서는:\n",
    "        # - 파일명/폴더 구조에서 라벨을 파싱하거나,\n",
    "        # - __init__에 labels 리스트/딕셔너리를 전달하거나,\n",
    "        # - CSV 파일을 읽어 라벨 맵을 만들어 사용해야 합니다.\n",
    "        # 라벨은 정수(int) 또는 torch.Tensor (예: torch.tensor(label, dtype=torch.long)) 로 반환하는 것이 일반적입니다.\n",
    "\n",
    "        return result, label\n",
    "        # 주의: 만약 self.transform이 None이면 result 변수가 정의되지 않아 NameError가 납니다.\n",
    "        # 따라서 transform None 케이스를 반드시 처리해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6effb542",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## 🟢 `CustomData`에 넣을 `인수` or `매개변수` 정의하기  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19134f7",
   "metadata": {},
   "source": [
    "\n",
    "### 🟡 **[transforms.Compose]** - 이미지 전처리 방법 정의하기  \n",
    "- 만들어놓은 CustomData class를 사용하기 전에 전처리 방법을 정의하자.  \n",
    "- 딥러닝 모델에 이미지를 넣기 전에, 모든 이미지의 크기를 같게 만들고, 딥러닝 모델이 계산하기 좋은 형태로 바꿔주는 등의 전처리 과정이 필요.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "81213a9a-3c33-4716-aa70-27dd0f6d7903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 가지 전처리 방법을 하나로 묶어주는 transforms.Compose를 사용합니다.\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        # 1. 이미지 크기 조절: 모든 이미지의 크기를 224x224 픽셀로 맞춥니다.\n",
    "        transforms.Resize((224, 224)),\n",
    "\n",
    "        # 2. 텐서(Tensor)로 변환: 이미지를 딥러닝 모델이 계산할 수 있는 숫자 행렬(텐서)로 바꿉니다.\n",
    "        transforms.ToTensor(),\n",
    "\n",
    "        # 3. 정규화(Normalize): 이미지의 픽셀 값 범위를 조정하여 모델이 더 빠르고 안정적으로 학습하도록 돕습니다.\n",
    "        #    아래 mean과 std 값은 ImageNet 데이터셋에서 미리 계산된 값으로, 보통 그대로 많이 사용합니다.\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3671a415",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 🟡 **[path 지정]** - (예시) 이미지 파일 경로 리스트 만들기  (❌ 데이터 없음)  \n",
    "- 🔥 path들을 리스트로 만들어한다는 것  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b8d9c60d-9ea2-4cf6-9eb9-55d50f45e009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bump', 'fall-down', 'fall-off', 'hit', 'jam', 'no-accident']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin = \"./data/safety-data/human-accident\" # 이미지 파일이 모여있는 directory를 가져옵니다.\n",
    "\n",
    "os.listdir(origin)  # 🔥 origin으로 정의한 directory에 존재하는 것들(ex. 이미지들, 영상들, 등등)을 리스트로 만들어줍니다!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1d5a136d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/safety-data/human-accident\\\\bump'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.listdir(origin)로 만든 리스트의 요소 하나씩을 가져와 origin path 뒤에 붙여서 다시 list로 만듭니다.\n",
    "image_list = [os.path.join(origin, x) for x in os.listdir(origin)]\n",
    "\n",
    "# 만든 image_list에서 첫 번째 파일의 전체 경로를 확인해 봅니다.\n",
    "image_list[0]    #  =  './data/safety-data/human-accident\\\\bump'   ...  windows에서는 '/' 이거말고, '\\\\'이걸로 써야겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d710660b-a4ae-4f1e-89e7-d67fede046f4",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "## 🟢 **[CustomData 사용]** - (예시) 이미지리스트를 내가 만든 데이터셋 클래스에 적용  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344353a6-9a3a-4c16-93ad-9238543b867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지에 대한 경로, 일괄적으로 적용할 전처리 이 2가지를 인수로 넣어줌.\n",
    "dataset = CustomData(image_list, transform) \n",
    "# 🔥🔥🔥 어떻게 작용하는 것일까?\n",
    "DATAS.DWWDWQD()\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,  # 🔥🔥🔥 어떻게 작용하는 것일까?\n",
    "    batch_size=25,\n",
    "    shuffle=True,\n",
    "    drop_last=False,  # 마지막에 남는 데이터도 사용합니다. (False: 버리지 않음)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76ee5e9",
   "metadata": {},
   "source": [
    "### 🟡 🔥🔥🔥🔥🔥 DataLoader에서 이미지 데이터 배치 꺼내보기  \n",
    "- DataLoader에서 이미지 데이터가 어떻게 배치 단위로 나오는지 확인해 보겠습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "100ad6bc-d298-4b59-b1de-99f4ea6af71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x0000019EBAD965D0>\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: './data/safety-data/human-accident\\\\jam'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m dataiter = \u001b[38;5;28miter\u001b[39m(dataloader) \u001b[38;5;66;03m# iterate \u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(dataiter)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataiter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bb\\Desktop\\learn_deeplearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bb\\Desktop\\learn_deeplearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bb\\Desktop\\learn_deeplearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bb\\Desktop\\learn_deeplearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mCustomData.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     22\u001b[39m image = \u001b[38;5;28mself\u001b[39m.image_path[idx]\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# image 변수는 '파일 경로 문자열'입니다. 주석과 혼동하면 안 됩니다(이미지가 아님).\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# 예: image == \"data/img_001.jpg\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m image = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 해당 경로에서 이미지를 읽어 옴\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# PIL.Image.open은 파일 핸들을 연 상태로 반환(지연 로딩)합니다.\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# 파일을 안전하게 닫으려면 with Image.open(path) as img: 같은 문법을 권장합니다.\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# 또한, 채널 통일을 위해 .convert('RGB') 혹은 .convert('L')을 호출하는 것이 안전합니다.\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bb\\Desktop\\learn_deeplearning\\.venv\\Lib\\site-packages\\PIL\\Image.py:3513\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3512\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3513\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3514\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3515\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: './data/safety-data/human-accident\\\\jam'"
     ]
    }
   ],
   "source": [
    "dataiter = iter(dataloader) # iterate \n",
    "print(dataiter)\n",
    "\n",
    "batch = next(dataiter)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b703b224",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa4057-6894-44cf-a8b2-58a446b43e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치에는 이미지와 라벨이 함께 들어있으므로, 각각을 images와 labels 변수에 저장.\n",
    "images, labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d22a0-819e-4731-a68f-5ac68804c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 배치에 들어있는 이미지와 라벨의 개수를 확인. (batch_size와 동일)\n",
    "len(images), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7354c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib로 첫 번째 데이터 배치 출력\n",
    "# 이미지가 배치 (N, C, H, W) 형태라고 가정 (예: N=25, C=3, H=224, W=224)\n",
    "def imshow(img, title=None):\n",
    "    \"\"\"이미지 시각화를 위한 함수\"\"\"\n",
    "    img = img.permute(1, 2, 0)  # (C, H, W) -> (H, W, C)\n",
    "    plt.imshow(img)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "\n",
    "# 배치의 첫 번째 이미지 출력\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(len(images)):\n",
    "    plt.subplot(5, 5, i + 1)  # 5x5 그리드 생성 (25개의 이미지)\n",
    "    imshow(images[i])\n",
    "    plt.title(f\"Label: {labels[i].item()}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefb5877",
   "metadata": {},
   "source": [
    "이 코드를 실행하면, DataLoader가 25개의 전처리된 이미지 데이터(텐서 형태)와 25개의 라벨을 하나의 batch로 묶어서 반환해주는 것을 확인할 수 있습니다.  \n",
    "이제 이 batch 데이터를 딥러닝 모델에 넣어서 학습을 진행하면 됩니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87a253",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "\n",
    "# 🟩 W & B (Weight & Biases)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee69ca70-2db5-4458-ab62-c0121c211818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f92f6d-7c12-4ff1-a0ad-19e577a0f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cff393-4dbc-4552-8183-f2e132d7f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"poth_resnet2\",  # 프로젝트 이름\n",
    "    #config.yaml\n",
    "    config={\n",
    "        \"epochs\": 5,\n",
    "        \"batch_size\": 25,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"model\": \"ResNet18\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b237961-98f7-403d-8bb4-14de61def7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet 모델 정의\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_classes = 2  # 데이터셋의 클래스 개수에 맞게 설정\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# GPU 사용 여부\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 🔥 손실 함수 및 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 모델 훈련\n",
    "num_epochs = wandb.config.epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)  # 데이터 GPU로 이동\n",
    "\n",
    "        optimizer.zero_grad()  # 옵티마이저 초기화\n",
    "        outputs = model(images)  # 순전파\n",
    "        loss = criterion(outputs, labels)  # 손실 계산\n",
    "        loss.backward()  # 역전파\n",
    "        optimizer.step()  # 가중치 업데이트\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # **WandB 배치 로깅**\n",
    "        wandb.log({\"batch_loss\": loss.item()})\n",
    "\n",
    "    # 에포크 종료 시 평균 손실 로그 기록\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    wandb.log({\"epoch_loss\": avg_loss, \"epoch\": epoch + 1})\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# 모델 저장 (선택 사항)\n",
    "torch.save(model.state_dict(), \"resnet18_poth_dataset.pth\")\n",
    "wandb.save(\"resnet18_poth_dataset.pth\")\n",
    "print(\"Model saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
