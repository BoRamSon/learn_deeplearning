import streamlit as st
import torch
import torch.nn as nn
import cv2
import numpy as np
from PIL import Image
import tempfile
import os
import sys
from pathlib import Path

# ë¡œì»¬ ëª¨ë“ˆ import
from cnnlstm_model import CNNLSTM
from torchvision import transforms
import config  # ì„¤ì • íŒŒì¼ import

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title=config.PAGE_TITLE,
    page_icon=config.PAGE_ICON,
    layout="wide"
)

@st.cache_resource
def load_model(model_path):
    """ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìºì‹œí•©ë‹ˆë‹¤."""
    device = torch.device("cuda" if torch.cuda.is_available() and not config.FORCE_CPU else "cpu")
    model = CNNLSTM(num_classes=config.NUM_CLASSES)
    
    if os.path.exists(model_path):
        try:
            checkpoint = torch.load(model_path, map_location=device)
            
            # ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° í™•ì¸ ë° ì²˜ë¦¬
            if isinstance(checkpoint, dict):
                # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ ê²½ìš°, 'model_state_dict' í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸
                if 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                elif 'state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['state_dict'])
                else:
                    # í‚¤ê°€ ì—†ìœ¼ë©´ ì „ì²´ë¥¼ state_dictë¡œ ê°„ì£¼
                    model.load_state_dict(checkpoint)
            else:
                # ì§ì ‘ state_dictì¸ ê²½ìš°
                model.load_state_dict(checkpoint)
            
            model.to(device)
            model.eval()
            st.success(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (Device: {device})")
            return model, device
            
        except Exception as e:
            st.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            
            # ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° ë””ë²„ê·¸ ì •ë³´ í‘œì‹œ
            try:
                checkpoint = torch.load(model_path, map_location='cpu')
                if isinstance(checkpoint, dict):
                    st.info(f"ğŸ” ì²´í¬í¬ì¸íŠ¸ í‚¤ë“¤: {list(checkpoint.keys())}")
                    if 'model_state_dict' in checkpoint:
                        st.info("ğŸ“¦ 'model_state_dict' í‚¤ ë°œê²¬")
                    elif 'state_dict' in checkpoint:
                        st.info("ğŸ“¦ 'state_dict' í‚¤ ë°œê²¬")
                else:
                    st.info("ğŸ“¦ ì²´í¬í¬ì¸íŠ¸ëŠ” ì§ì ‘ state_dict í˜•íƒœì…ë‹ˆë‹¤")
            except:
                pass
            
            return None, device
    else:
        st.error(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        return None, device

def preprocess_video(video_path, fixed_len=None, target_fps=None):
    """ë¹„ë””ì˜¤ë¥¼ ì „ì²˜ë¦¬í•˜ì—¬ ëª¨ë¸ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    
    # ê¸°ë³¸ê°’ ì„¤ì •
    if fixed_len is None:
        fixed_len = config.FIXED_SEQUENCE_LENGTH
    if target_fps is None:
        target_fps = config.TARGET_FPS
    
    # Transform ì •ì˜ (í•™ìŠµ ì‹œì™€ ë™ì¼)
    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize(config.INPUT_SIZE),
        transforms.ToTensor(),
        transforms.Normalize(
            mean=config.IMAGENET_MEAN,
            std=config.IMAGENET_STD
        ),
    ])
    
    # ë¹„ë””ì˜¤ ì½ê¸°
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        st.error("ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # FPS ì •ë³´
    original_fps = int(cap.get(cv2.CAP_PROP_FPS))
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    st.info(f"ğŸ“¹ ì›ë³¸ FPS: {original_fps}, ì´ í”„ë ˆì„: {frame_count}")
    
    # í”„ë ˆì„ ì¶”ì¶œ (target_fpsë¡œ ë¦¬ìƒ˜í”Œë§)
    frames = []
    frame_interval = max(1, original_fps // target_fps)
    
    frame_idx = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break
            
        if frame_idx % frame_interval == 0:
            # BGR -> RGB ë³€í™˜
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(frame_rgb)
            
        frame_idx += 1
    
    cap.release()
    
    if len(frames) == 0:
        st.error("í”„ë ˆì„ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    st.info(f"ğŸ¬ ì¶”ì¶œëœ í”„ë ˆì„ ìˆ˜: {len(frames)}")
    
    # ê¸¸ì´ ê³ ì • (16í”„ë ˆì„)
    if len(frames) >= fixed_len:
        # ê· ë“± ìƒ˜í”Œë§
        indices = np.linspace(0, len(frames) - 1, fixed_len, dtype=int)
        frames = [frames[i] for i in indices]
    else:
        # íŒ¨ë”© (ë§ˆì§€ë§‰ í”„ë ˆì„ ë°˜ë³µ)
        while len(frames) < fixed_len:
            frames.append(frames[-1])
    
    # Transform ì ìš©
    processed_frames = []
    for frame in frames:
        tensor_frame = transform(frame)  # (C, H, W)
        processed_frames.append(tensor_frame)
    
    # í…ì„œë¡œ ë³€í™˜: (T, C, H, W)
    video_tensor = torch.stack(processed_frames)
    
    # ë°°ì¹˜ ì°¨ì› ì¶”ê°€: (1, T, C, H, W)
    video_tensor = video_tensor.unsqueeze(0)
    
    return video_tensor, frames

def predict_video(model, video_tensor, device):
    """ëª¨ë¸ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    with torch.no_grad():
        video_tensor = video_tensor.to(device)
        logits = model(video_tensor)  # (1, num_classes)
        probabilities = torch.softmax(logits, dim=1)
        predicted_class = torch.argmax(probabilities, dim=1).item()
        confidence = probabilities[0, predicted_class].item()
        
    return predicted_class, confidence, probabilities[0].cpu().numpy()

def main():
    st.title(config.PAGE_TITLE)
    st.markdown("---")
    
    # ì‚¬ì´ë“œë°” - ëª¨ë¸ ì„¤ì •
    st.sidebar.header("âš™ï¸ ëª¨ë¸ ì„¤ì •")
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ í‘œì‹œ
    available_models = config.get_available_models()
    
    if available_models:
        st.sidebar.subheader("ğŸ“ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸")
        
        # ëª¨ë¸ ì„ íƒ ë“œë¡­ë‹¤ìš´
        model_options = [f"{m['name']} ({m['size_mb']}MB)" for m in available_models]
        selected_idx = st.sidebar.selectbox(
            "ëª¨ë¸ ì„ íƒ",
            range(len(model_options)),
            format_func=lambda x: model_options[x],
            help="í•™ìŠµëœ ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”"
        )
        model_path = available_models[selected_idx]['path']
        
        # ì„ íƒëœ ëª¨ë¸ ì •ë³´ í‘œì‹œ
        selected_model = available_models[selected_idx]
        st.sidebar.info(f"âœ… ì„ íƒëœ ëª¨ë¸: {selected_model['name']}")
        
    else:
        st.sidebar.warning("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        model_path = st.sidebar.text_input(
            "ëª¨ë¸ íŒŒì¼ ê²½ë¡œ", 
            value=config.get_model_path(),
            help="í•™ìŠµëœ .pth íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”"
        )
    
    # ê²½ë¡œ ìƒíƒœ í™•ì¸
    paths_status = config.validate_paths()
    with st.sidebar.expander("ğŸ” ê²½ë¡œ ìƒíƒœ í™•ì¸"):
        for path_name, exists in paths_status.items():
            status_icon = "âœ…" if exists else "âŒ"
            st.text(f"{status_icon} {path_name}: {'ì¡´ì¬' if exists else 'ì—†ìŒ'}")
    
    # ëª¨ë¸ ë¡œë“œ
    model, device = load_model(model_path)
    
    if model is None:
        st.warning("âš ï¸ ëª¨ë¸ì„ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.")
        st.info("ğŸ’¡ í•™ìŠµì´ ì™„ë£Œëœ í›„ `snapshots/best.pth` íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤.")
        
        # í•™ìŠµ ì§„í–‰ ì•ˆë‚´
        with st.expander("ğŸ“ ëª¨ë¸ í•™ìŠµ ë°©ë²•"):
            st.markdown("""
            **1. í•™ìŠµ ì‹¤í–‰ ëª…ë ¹:**
            ```powershell
            cd human-accident\\cnn-lstm
            uv run python main.py --root "C:\\Users\\bb\\Desktop\\learn_deeplearning\\human-accident\\data\\safety-data\\human-accident" --epochs 10 --batch-size 8 --lr 1e-4 --weight-decay 1e-4 --num-workers 2 --train-ratio 0.7 --seed 42 --num-classes 6
            ```
            
            **2. í•™ìŠµ ì™„ë£Œ í™•ì¸:**
            - `snapshots/` í´ë”ì— `best.pth` íŒŒì¼ì´ ìƒì„±ë¨
            - ì½˜ì†”ì— "Best model saved!" ë©”ì‹œì§€ ì¶œë ¥
            
            **3. ì˜ˆìƒ í•™ìŠµ ì‹œê°„:**
            - GPU ì‚¬ìš© ì‹œ: ì•½ 30ë¶„-1ì‹œê°„ (ì—í­ ìˆ˜ì— ë”°ë¼)
            - CPU ì‚¬ìš© ì‹œ: ì•½ 2-4ì‹œê°„
            
            **4. í•™ìŠµ ì¤‘ í™•ì¸ì‚¬í•­:**
            - tqdm ì§„í–‰ë°”ë¡œ ì‹¤ì‹œê°„ ì§„í–‰ìƒí™© í™•ì¸
            - GPU ì‚¬ìš© ì—¬ë¶€ ë””ë²„ê·¸ ë¡œê·¸ í™•ì¸
            - ê° ì—í­ë³„ train/val ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            """)
        
        # í˜„ì¬ snapshots í´ë” ìƒíƒœ í‘œì‹œ
        snapshots_path = Path(config.get_snapshots_dir())
        if snapshots_path.exists():
            files = list(snapshots_path.glob("*"))
            st.info(f"ğŸ“ í˜„ì¬ snapshots í´ë”: {len(files)}ê°œ íŒŒì¼")
            if files:
                for file in files:
                    st.text(f"  - {file.name}")
        
        return
    
    # ë©”ì¸ ì˜ì—­
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.header("ğŸ“¤ ë¹„ë””ì˜¤ ì—…ë¡œë“œ")
        
        uploaded_file = st.file_uploader(
            "ë¶„ì„í•  ë¹„ë””ì˜¤ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”",
            type=config.SUPPORTED_VIDEO_FORMATS,
            help=f"ì§€ì› í˜•ì‹: {', '.join(config.SUPPORTED_VIDEO_FORMATS).upper()}"
        )
        
        if uploaded_file is not None:
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            with tempfile.NamedTemporaryFile(delete=False, suffix=f".{uploaded_file.name.split('.')[-1]}") as tmp_file:
                tmp_file.write(uploaded_file.read())
                temp_video_path = tmp_file.name
            
            # ë¹„ë””ì˜¤ í‘œì‹œ
            st.video(uploaded_file)
            
            # ë¶„ì„ ë²„íŠ¼
            if st.button("ğŸ” ì•ˆì „ì‚¬ê³  ë¶„ì„ ì‹œì‘", type="primary"):
                with st.spinner("ë¹„ë””ì˜¤ ë¶„ì„ ì¤‘..."):
                    # ì „ì²˜ë¦¬
                    result = preprocess_video(temp_video_path)
                    if result is None:
                        return
                    
                    video_tensor, frames = result
                    
                    # ì˜ˆì¸¡
                    pred_class, confidence, all_probs = predict_video(model, video_tensor, device)
                    
                    # ê²°ê³¼ í‘œì‹œ
                    with col2:
                        st.header("ğŸ“Š ë¶„ì„ ê²°ê³¼")
                        
                        # ì˜ˆì¸¡ ê²°ê³¼
                        predicted_label_kr = config.CLASS_NAMES_KR[pred_class]
                        predicted_label_en = config.CLASS_NAMES[pred_class]
                        
                        if pred_class == 5:  # no-accident
                            st.success(f"âœ… **{predicted_label_kr}**")
                            st.success(f"ì‹ ë¢°ë„: **{confidence:.2%}**")
                        else:
                            st.error(f"âš ï¸ **{predicted_label_kr} ê°ì§€!**")
                            st.error(f"ì‹ ë¢°ë„: **{confidence:.2%}**")
                        
                        st.info(f"ì˜ë¬¸ í´ë˜ìŠ¤: `{predicted_label_en}`")
                        
                        # ëª¨ë“  í´ë˜ìŠ¤ë³„ í™•ë¥ 
                        st.subheader("ğŸ“ˆ í´ë˜ìŠ¤ë³„ í™•ë¥ ")
                        
                        # ë°” ì°¨íŠ¸
                        st.bar_chart(dict(zip(config.CLASS_NAMES_KR, all_probs)))
                        
                        # í…Œì´ë¸”
                        import pandas as pd
                        df = pd.DataFrame({
                            "í´ë˜ìŠ¤": config.CLASS_NAMES_KR,
                            "ì˜ë¬¸": config.CLASS_NAMES,
                            "í™•ë¥ ": [f"{prob:.2%}" for prob in all_probs]
                        })
                        st.dataframe(df, width='stretch')
                        
                        # ìƒ˜í”Œ í”„ë ˆì„ í‘œì‹œ
                        st.subheader("ğŸ¬ ë¶„ì„ëœ í”„ë ˆì„ ìƒ˜í”Œ")
                        
                        # 4ê°œ í”„ë ˆì„ë§Œ í‘œì‹œ
                        sample_indices = [0, 5, 10, 15]
                        frame_cols = st.columns(4)
                        
                        for i, idx in enumerate(sample_indices):
                            if idx < len(frames):
                                with frame_cols[i]:
                                    st.image(
                                        frames[idx], 
                                        caption=f"í”„ë ˆì„ {idx+1}",
                                        width='stretch'
                                    )
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            try:
                os.unlink(temp_video_path)
            except:
                pass
    
    # í•˜ë‹¨ ì •ë³´
    st.markdown("---")
    with st.expander("â„¹ï¸ ì‹œìŠ¤í…œ ì •ë³´"):
        st.markdown(f"""
        **ëª¨ë¸ ì •ë³´:**
        - ì•„í‚¤í…ì²˜: CNN-LSTM
        - ë°±ë³¸: ResNet-101 (ì‚¬ì „í•™ìŠµ)
        - ì…ë ¥ í˜•íƒœ: 16í”„ë ˆì„ Ã— 224Ã—224 RGB
        - í´ë˜ìŠ¤ ìˆ˜: 6ê°œ
        - ë””ë°”ì´ìŠ¤: {device}
        
        **ì§€ì› ì‚¬ê³  ìœ í˜•:**
        - ì¶©ëŒ ì‚¬ê³  (bump)
        - ë„˜ì–´ì§ ì‚¬ê³  (fall-down)  
        - ì¶”ë½ ì‚¬ê³  (fall-off)
        - íƒ€ê²© ì‚¬ê³  (hit)
        - ë¼ì„ ì‚¬ê³  (jam)
        - ì •ìƒ ìƒí™© (no-accident)
        """)

if __name__ == "__main__":
    main()
