{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "26G7ex5U4XNo"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 7장. 합성곱 신경망(CNN)"
      ],
      "metadata": {
        "id": "1Sd-PF6l0Vei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예제 코드용 공통 로직\n",
        "\n",
        "[노트] 파일 분리 후 임포트하도록 수정 예정입니다."
      ],
      "metadata": {
        "id": "26G7ex5U4XNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dataset/mnist.py"
      ],
      "metadata": {
        "id": "2xwwvmge2w6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import os\n",
        "import os.path\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "save_file = os.getcwd() + \"/mnist.pkl\"\n",
        "\n",
        "def _convert_numpy():\n",
        "    dataset = {}\n",
        "\n",
        "    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "    # 이미지 데이터 평탄화\n",
        "    dataset['train_img'] = train_images.reshape(train_images.shape[0], -1)\n",
        "    dataset['train_label'] = train_labels\n",
        "    dataset['test_img'] = test_images.reshape(test_images.shape[0], -1)\n",
        "    dataset['test_label'] = test_labels\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def init_mnist():\n",
        "    dataset = _convert_numpy()\n",
        "    print(\"Creating pickle file ...\")\n",
        "    with open(save_file, 'wb') as f:\n",
        "        pickle.dump(dataset, f, -1)\n",
        "    print(\"Done!\")\n",
        "\n",
        "def _change_one_hot_label(X):\n",
        "    T = np.zeros((X.size, 10))\n",
        "    for idx, row in enumerate(T):\n",
        "        row[X[idx]] = 1\n",
        "\n",
        "    return T\n",
        "\n",
        "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
        "    \"\"\"MNIST 데이터셋 읽기\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    normalize : 이미지의 픽셀 값을 0.0~1.0 사이의 값으로 정규화할지 정한다.\n",
        "    one_hot_label :\n",
        "        one_hot_label이 True면、레이블을 원-핫(one-hot) 배열로 돌려준다.\n",
        "        one-hot 배열은 예를 들어 [0,0,1,0,0,0,0,0,0,0]처럼 한 원소만 1인 배열이다.\n",
        "    flatten : 입력 이미지를 1차원 배열로 만들지를 정한다.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (훈련 이미지, 훈련 레이블), (시험 이미지, 시험 레이블)\n",
        "    \"\"\"\n",
        "    if not os.path.exists(save_file):\n",
        "        init_mnist()\n",
        "\n",
        "    with open(save_file, 'rb') as f:\n",
        "        dataset = pickle.load(f)\n",
        "\n",
        "    if normalize:\n",
        "        for key in ('train_img', 'test_img'):\n",
        "            dataset[key] = dataset[key].astype(np.float32)\n",
        "            dataset[key] /= 255.0\n",
        "\n",
        "    if one_hot_label:\n",
        "        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n",
        "        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])\n",
        "\n",
        "    if not flatten:\n",
        "         for key in ('train_img', 'test_img'):\n",
        "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
        "\n",
        "    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label'])"
      ],
      "metadata": {
        "id": "DVvs8c0d201u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### common/functions.py"
      ],
      "metadata": {
        "id": "0yc1izsA4ZEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T\n",
        "\n",
        "    x = x - np.max(x) # 오버플로 대책\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "\n",
        "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "\n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
      ],
      "metadata": {
        "id": "gdDRxOpd4dSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### common/util.py"
      ],
      "metadata": {
        "id": "I-ii8wwz5bZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def smooth_curve(x):\n",
        "    \"\"\"손실 함수의 그래프를 매끄럽게 하기 위해 사용\n",
        "\n",
        "    참고：http://glowingpython.blogspot.jp/2012/02/convolution-with-numpy.html\n",
        "    \"\"\"\n",
        "    window_len = 11\n",
        "    s = np.r_[x[window_len-1:0:-1], x, x[-1:-window_len:-1]]\n",
        "    w = np.kaiser(window_len, 2)\n",
        "    y = np.convolve(w/w.sum(), s, mode='valid')\n",
        "    return y[5:len(y)-5]\n",
        "\n",
        "\n",
        "def shuffle_dataset(x, t):\n",
        "    \"\"\"데이터셋을 뒤섞는다.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : 훈련 데이터\n",
        "    t : 정답 레이블\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    x, t : 뒤섞은 훈련 데이터와 정답 레이블\n",
        "    \"\"\"\n",
        "    permutation = np.random.permutation(x.shape[0])\n",
        "    x = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]\n",
        "    t = t[permutation]\n",
        "\n",
        "    return x, t\n",
        "\n",
        "def conv_output_size(input_size, filter_size, stride=1, pad=0):\n",
        "    return (input_size + 2*pad - filter_size) / stride + 1\n",
        "\n",
        "\n",
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
        "    filter_h : 필터의 높이\n",
        "    filter_w : 필터의 너비\n",
        "    stride : 스트라이드\n",
        "    pad : 패딩\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    col : 2차원 배열\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_data.shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "\n",
        "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
        "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "    return col\n",
        "\n",
        "\n",
        "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    col : 2차원 배열(입력 데이터)\n",
        "    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n",
        "    filter_h : 필터의 높이\n",
        "    filter_w : 필터의 너비\n",
        "    stride : 스트라이드\n",
        "    pad : 패딩\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    img : 변환된 이미지들\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
        "\n",
        "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
        "\n",
        "    return img[:, :, pad:H + pad, pad:W + pad]"
      ],
      "metadata": {
        "id": "wVJZrI1C5c8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### common/layers.py"
      ],
      "metadata": {
        "id": "qP0cMMnE5ekw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#from common.functions import *\n",
        "#from common.util import im2col, col2im\n",
        "\n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "\n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        # 가중치와 편향 매개변수의 미분\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 텐서 대응\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "\n",
        "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None # 손실함수\n",
        "        self.y = None    # softmax의 출력\n",
        "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Dropout:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1207.0580\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        if train_flg:\n",
        "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "\n",
        "\n",
        "class BatchNormalization:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1502.03167\n",
        "    \"\"\"\n",
        "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "        self.momentum = momentum\n",
        "        self.input_shape = None # 합성곱 계층은 4차원, 완전연결 계층은 2차원\n",
        "\n",
        "        # 시험할 때 사용할 평균과 분산\n",
        "        self.running_mean = running_mean\n",
        "        self.running_var = running_var\n",
        "\n",
        "        # backward 시에 사용할 중간 데이터\n",
        "        self.batch_size = None\n",
        "        self.xc = None\n",
        "        self.std = None\n",
        "        self.dgamma = None\n",
        "        self.dbeta = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        self.input_shape = x.shape\n",
        "        if x.ndim != 2:\n",
        "            N, C, H, W = x.shape\n",
        "            x = x.reshape(N, -1)\n",
        "\n",
        "        out = self.__forward(x, train_flg)\n",
        "\n",
        "        return out.reshape(*self.input_shape)\n",
        "\n",
        "    def __forward(self, x, train_flg):\n",
        "        if self.running_mean is None:\n",
        "            N, D = x.shape\n",
        "            self.running_mean = np.zeros(D)\n",
        "            self.running_var = np.zeros(D)\n",
        "\n",
        "        if train_flg:\n",
        "            mu = x.mean(axis=0)\n",
        "            xc = x - mu\n",
        "            var = np.mean(xc**2, axis=0)\n",
        "            std = np.sqrt(var + 10e-7)\n",
        "            xn = xc / std\n",
        "\n",
        "            self.batch_size = x.shape[0]\n",
        "            self.xc = xc\n",
        "            self.xn = xn\n",
        "            self.std = std\n",
        "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
        "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var\n",
        "        else:\n",
        "            xc = x - self.running_mean\n",
        "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
        "\n",
        "        out = self.gamma * xn + self.beta\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        if dout.ndim != 2:\n",
        "            N, C, H, W = dout.shape\n",
        "            dout = dout.reshape(N, -1)\n",
        "\n",
        "        dx = self.__backward(dout)\n",
        "\n",
        "        dx = dx.reshape(*self.input_shape)\n",
        "        return dx\n",
        "\n",
        "    def __backward(self, dout):\n",
        "        dbeta = dout.sum(axis=0)\n",
        "        dgamma = np.sum(self.xn * dout, axis=0)\n",
        "        dxn = self.gamma * dout\n",
        "        dxc = dxn / self.std\n",
        "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
        "        dvar = 0.5 * dstd / self.std\n",
        "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
        "        dmu = np.sum(dxc, axis=0)\n",
        "        dx = dxc - dmu / self.batch_size\n",
        "\n",
        "        self.dgamma = dgamma\n",
        "        self.dbeta = dbeta\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "kQfRqz2p5gDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### common/gradient.py"
      ],
      "metadata": {
        "id": "2xD1J8NJ5FRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def _numerical_gradient_1d(f, x):\n",
        "    h = 1e-4 # 0.0001\n",
        "    grad = np.zeros_like(x)\n",
        "\n",
        "    for idx in range(x.size):\n",
        "        tmp_val = x[idx]\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x) # f(x+h)\n",
        "\n",
        "        x[idx] = tmp_val - h\n",
        "        fxh2 = f(x) # f(x-h)\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "\n",
        "        x[idx] = tmp_val # 값 복원\n",
        "\n",
        "    return grad\n",
        "\n",
        "def numerical_gradient_2d(f, X):\n",
        "    if X.ndim == 1:\n",
        "        return _numerical_gradient_1d(f, X)\n",
        "    else:\n",
        "        grad = np.zeros_like(X)\n",
        "\n",
        "        for idx, x in enumerate(X):\n",
        "            grad[idx] = _numerical_gradient_1d(f, x)\n",
        "\n",
        "        return grad\n",
        "\n",
        "def numerical_gradient(f, x):\n",
        "    h = 1e-4 # 0.0001\n",
        "    grad = np.zeros_like(x)\n",
        "\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        tmp_val = x[idx]\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x) # f(x+h)\n",
        "\n",
        "        x[idx] = tmp_val - h\n",
        "        fxh2 = f(x) # f(x-h)\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "\n",
        "        x[idx] = tmp_val # 값 복원\n",
        "        it.iternext()\n",
        "\n",
        "    return grad"
      ],
      "metadata": {
        "id": "RL_GmMDc5HQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### common/optimizer.py"
      ],
      "metadata": {
        "id": "cHpX3u3J3CWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    \"\"\"확률적 경사 하강법（Stochastic Gradient Descent）\"\"\"\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        for key in params.keys():\n",
        "            params[key] -= self.lr * grads[key]\n",
        "\n",
        "class Momentum:\n",
        "    \"\"\"모멘텀 SGD\"\"\"\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\n",
        "            params[key] += self.v[key]\n",
        "\n",
        "class Nesterov:\n",
        "    \"\"\"Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\"\"\"\n",
        "    # NAG는 모멘텀에서 한 단계 발전한 방법이다. (http://newsight.tistory.com/224)\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.v[key] *= self.momentum\n",
        "            self.v[key] -= self.lr * grads[key]\n",
        "            params[key] += self.momentum * self.momentum * self.v[key]\n",
        "            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n",
        "\n",
        "class AdaGrad:\n",
        "    \"\"\"AdaGrad\"\"\"\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.h[key] += grads[key] * grads[key]\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
        "\n",
        "class RMSprop:\n",
        "    \"\"\"RMSprop\"\"\"\n",
        "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
        "        self.lr = lr\n",
        "        self.decay_rate = decay_rate\n",
        "        self.h = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.h[key] *= self.decay_rate\n",
        "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
        "\n",
        "class Adam:\n",
        "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.iter = 0\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m, self.v = {}, {}\n",
        "            for key, val in params.items():\n",
        "                self.m[key] = np.zeros_like(val)\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        self.iter += 1\n",
        "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
        "\n",
        "        for key in params.keys():\n",
        "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
        "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
        "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
        "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
        "\n",
        "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
        "\n",
        "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
        "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
        "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)"
      ],
      "metadata": {
        "id": "LlSPtbxv3Csl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### common/trainer.py"
      ],
      "metadata": {
        "id": "70AqxvtS3F9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#from common.optimizer import *\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"신경망 훈련을 대신 해주는 클래스\n",
        "    \"\"\"\n",
        "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
        "                 epochs=20, mini_batch_size=100,\n",
        "                 optimizer='SGD', optimizer_param={'lr':0.01},\n",
        "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
        "        self.network = network\n",
        "        self.verbose = verbose\n",
        "        self.x_train = x_train\n",
        "        self.t_train = t_train\n",
        "        self.x_test = x_test\n",
        "        self.t_test = t_test\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = mini_batch_size\n",
        "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
        "\n",
        "        # optimzer\n",
        "        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
        "                                'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}\n",
        "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
        "\n",
        "        self.train_size = x_train.shape[0]\n",
        "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
        "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
        "        self.current_iter = 0\n",
        "        self.current_epoch = 0\n",
        "\n",
        "        self.train_loss_list = []\n",
        "        self.train_acc_list = []\n",
        "        self.test_acc_list = []\n",
        "\n",
        "    def train_step(self):\n",
        "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
        "        x_batch = self.x_train[batch_mask]\n",
        "        t_batch = self.t_train[batch_mask]\n",
        "\n",
        "        grads = self.network.gradient(x_batch, t_batch)\n",
        "        self.optimizer.update(self.network.params, grads)\n",
        "\n",
        "        loss = self.network.loss(x_batch, t_batch)\n",
        "        self.train_loss_list.append(loss)\n",
        "        if self.verbose: print(\"train loss:\" + str(loss))\n",
        "\n",
        "        if self.current_iter % self.iter_per_epoch == 0:\n",
        "            self.current_epoch += 1\n",
        "\n",
        "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
        "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
        "            if not self.evaluate_sample_num_per_epoch is None:\n",
        "                t = self.evaluate_sample_num_per_epoch\n",
        "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
        "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
        "\n",
        "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
        "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
        "            self.train_acc_list.append(train_acc)\n",
        "            self.test_acc_list.append(test_acc)\n",
        "\n",
        "            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
        "        self.current_iter += 1\n",
        "\n",
        "    def train(self):\n",
        "        for i in range(self.max_iter):\n",
        "            self.train_step()\n",
        "\n",
        "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"=============== Final Test Accuracy ===============\")\n",
        "            print(\"test acc:\" + str(test_acc))"
      ],
      "metadata": {
        "id": "28liIffk3H0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***common/util.py***"
      ],
      "metadata": {
        "id": "SZr9Lt5s1Hdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
        "    filter_h : 필터의 높이\n",
        "    filter_w : 필터의 너비\n",
        "    stride : 스트라이드\n",
        "    pad : 패딩\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    col : 2차원 배열\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_data.shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "\n",
        "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
        "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "    return col"
      ],
      "metadata": {
        "id": "MkNBKF9H1MnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.4 합성곱/풀링 계층 구현하기"
      ],
      "metadata": {
        "id": "nrUx6w7s0eUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4.1 4차원 배열"
      ],
      "metadata": {
        "id": "WIkDqwF4009z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.random.rand(10, 1, 28, 28)  # 무작위로 데이터 생성\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31OsBnDt0hj6",
        "outputId": "0679d967-d285-4357-b907-94fa37fcd08b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngyqLEga0qzo",
        "outputId": "c6d7f16e-2d2b-46eb-da42-3cbf69d171ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKz1uHsq0sp8",
        "outputId": "20b8fd99-3052-4658-ad49-a87404c585c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0, 0]  # 또는 x[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaxQxYwp0vFl",
        "outputId": "72071f0c-26f1-43ce-d5a0-d0d6041bf1c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.79547242, 0.50479942, 0.67907922, 0.08282503, 0.37145712,\n",
              "        0.00978024, 0.49107956, 0.87955319, 0.63272118, 0.7157224 ,\n",
              "        0.96795018, 0.84282998, 0.35919166, 0.24708611, 0.8085778 ,\n",
              "        0.65742937, 0.2127302 , 0.96597011, 0.18132437, 0.65726879,\n",
              "        0.44952091, 0.72673824, 0.10246724, 0.28641892, 0.20922148,\n",
              "        0.44346643, 0.64574567, 0.25306127],\n",
              "       [0.43157697, 0.16271159, 0.00311236, 0.31729777, 0.03839849,\n",
              "        0.48990168, 0.10456166, 0.85877207, 0.3056989 , 0.90173171,\n",
              "        0.83776925, 0.93029229, 0.8125006 , 0.55835639, 0.49914043,\n",
              "        0.59536861, 0.70837836, 0.11054559, 0.11221602, 0.40257135,\n",
              "        0.16963962, 0.69158237, 0.03668317, 0.7490375 , 0.65517223,\n",
              "        0.37682927, 0.28588768, 0.83025422],\n",
              "       [0.16704005, 0.28112779, 0.94502815, 0.9780881 , 0.42008751,\n",
              "        0.40095105, 0.92648348, 0.17766979, 0.05276666, 0.84611077,\n",
              "        0.4503963 , 0.16697737, 0.96210975, 0.32163476, 0.78805678,\n",
              "        0.09635626, 0.60839643, 0.83001271, 0.18679424, 0.68939582,\n",
              "        0.59936942, 0.8558119 , 0.06061659, 0.75835589, 0.07811508,\n",
              "        0.22019653, 0.41520532, 0.47400939],\n",
              "       [0.00278704, 0.73460617, 0.51657514, 0.10513482, 0.98168254,\n",
              "        0.4752312 , 0.52401399, 0.17522136, 0.24441738, 0.60069944,\n",
              "        0.45690129, 0.69709692, 0.94557316, 0.50304362, 0.55344251,\n",
              "        0.80067503, 0.54606185, 0.35407892, 0.42719528, 0.29255849,\n",
              "        0.71925759, 0.13020232, 0.39668953, 0.51405986, 0.45826915,\n",
              "        0.56569697, 0.48089081, 0.64412071],\n",
              "       [0.80697124, 0.0389963 , 0.99659817, 0.15220269, 0.81768327,\n",
              "        0.0413832 , 0.3810452 , 0.16121453, 0.04973165, 0.71664369,\n",
              "        0.45447561, 0.17497899, 0.73189019, 0.85323737, 0.33834356,\n",
              "        0.60382427, 0.72411879, 0.1736099 , 0.24439197, 0.79131268,\n",
              "        0.14001974, 0.48831159, 0.17760584, 0.51810391, 0.19946016,\n",
              "        0.18860301, 0.30994235, 0.61362633],\n",
              "       [0.05539677, 0.76267466, 0.76692883, 0.00540663, 0.81374159,\n",
              "        0.12577169, 0.22433715, 0.152215  , 0.81461846, 0.9843943 ,\n",
              "        0.47463185, 0.94568816, 0.15166377, 0.56268451, 0.23870644,\n",
              "        0.61597225, 0.30550215, 0.43933867, 0.13399971, 0.47394417,\n",
              "        0.11762105, 0.85617173, 0.60746654, 0.12388322, 0.44159203,\n",
              "        0.04065605, 0.22027599, 0.47330832],\n",
              "       [0.28245568, 0.1575381 , 0.73780556, 0.78112386, 0.02892963,\n",
              "        0.42140452, 0.04992048, 0.17893609, 0.35937183, 0.43436997,\n",
              "        0.8973268 , 0.66387245, 0.2657957 , 0.54213728, 0.41718982,\n",
              "        0.5721014 , 0.30459288, 0.01067161, 0.48524518, 0.558787  ,\n",
              "        0.41109566, 0.09550627, 0.66772295, 0.88192587, 0.38556112,\n",
              "        0.21420729, 0.56023052, 0.55668336],\n",
              "       [0.04347094, 0.5543492 , 0.11978125, 0.26513862, 0.55170809,\n",
              "        0.86735332, 0.51288673, 0.0168421 , 0.17788376, 0.68309158,\n",
              "        0.10295022, 0.72941717, 0.31823727, 0.94337011, 0.1814626 ,\n",
              "        0.31347458, 0.57643154, 0.35517058, 0.20708509, 0.05577379,\n",
              "        0.88947753, 0.68326864, 0.46968573, 0.70328171, 0.38127622,\n",
              "        0.36753204, 0.53557883, 0.47778983],\n",
              "       [0.02582979, 0.49164643, 0.89118258, 0.06274437, 0.5250119 ,\n",
              "        0.81075032, 0.11615542, 0.41105584, 0.37905449, 0.77738202,\n",
              "        0.0231511 , 0.39334946, 0.74495767, 0.61742991, 0.73725272,\n",
              "        0.62497353, 0.25239928, 0.88305258, 0.54201932, 0.21131921,\n",
              "        0.12278538, 0.09698603, 0.93685436, 0.46845024, 0.7092527 ,\n",
              "        0.51689219, 0.76333057, 0.92382853],\n",
              "       [0.37524081, 0.14681841, 0.74505957, 0.18881963, 0.30776186,\n",
              "        0.88417562, 0.94623633, 0.7554373 , 0.28608421, 0.781665  ,\n",
              "        0.67342171, 0.33577488, 0.61833178, 0.90884417, 0.96358787,\n",
              "        0.07495107, 0.47625901, 0.3486725 , 0.99779703, 0.22097214,\n",
              "        0.55779925, 0.10839616, 0.83014284, 0.1413377 , 0.36530228,\n",
              "        0.30121327, 0.5937865 , 0.47029108],\n",
              "       [0.01047459, 0.13013222, 0.86992487, 0.64477333, 0.50826494,\n",
              "        0.31956188, 0.87686757, 0.30522532, 0.59082437, 0.51408997,\n",
              "        0.9664868 , 0.4178332 , 0.13506645, 0.90358229, 0.58900535,\n",
              "        0.70283504, 0.40496913, 0.56578431, 0.97499632, 0.13065298,\n",
              "        0.35713891, 0.12642504, 0.53239155, 0.97773392, 0.30165085,\n",
              "        0.87808459, 0.47746602, 0.39832609],\n",
              "       [0.27619508, 0.11945687, 0.0977869 , 0.25459572, 0.08008783,\n",
              "        0.92935783, 0.46496498, 0.81461389, 0.4176069 , 0.46617739,\n",
              "        0.31761874, 0.25749335, 0.38406107, 0.02020836, 0.51781265,\n",
              "        0.28126583, 0.86795503, 0.00767706, 0.37406461, 0.04417525,\n",
              "        0.51446825, 0.17319375, 0.05302424, 0.80807763, 0.78122576,\n",
              "        0.10947135, 0.92200491, 0.17761031],\n",
              "       [0.48605151, 0.42383621, 0.56530901, 0.51542767, 0.87619323,\n",
              "        0.94429271, 0.2387634 , 0.92935292, 0.09522653, 0.43148587,\n",
              "        0.97844709, 0.76683159, 0.400413  , 0.17470153, 0.90141514,\n",
              "        0.40769813, 0.31644883, 0.85117036, 0.85363354, 0.23076807,\n",
              "        0.24848999, 0.95185418, 0.39719973, 0.40699002, 0.7128701 ,\n",
              "        0.90991403, 0.60315867, 0.20354362],\n",
              "       [0.39110481, 0.49239457, 0.60890237, 0.35348593, 0.72629088,\n",
              "        0.99912241, 0.67711394, 0.53756271, 0.30158596, 0.94227903,\n",
              "        0.6490398 , 0.33560629, 0.8879551 , 0.08836732, 0.9918048 ,\n",
              "        0.93664928, 0.45960307, 0.22281123, 0.0678917 , 0.23205255,\n",
              "        0.91757421, 0.6542361 , 0.70598876, 0.78988286, 0.84407069,\n",
              "        0.66527022, 0.08807596, 0.87670666],\n",
              "       [0.19663543, 0.96787834, 0.77961688, 0.33486327, 0.90777791,\n",
              "        0.82946816, 0.51273291, 0.55577145, 0.14778548, 0.7445964 ,\n",
              "        0.60548259, 0.45253981, 0.40625994, 0.84173487, 0.4273088 ,\n",
              "        0.24266794, 0.29079575, 0.71424342, 0.88838239, 0.41989678,\n",
              "        0.68354545, 0.61921731, 0.07898394, 0.73195831, 0.37071234,\n",
              "        0.02070952, 0.30331059, 0.32014985],\n",
              "       [0.52654171, 0.03390143, 0.58660469, 0.16226352, 0.01954558,\n",
              "        0.53101671, 0.61106484, 0.75593213, 0.35042569, 0.62193117,\n",
              "        0.96724309, 0.04010953, 0.37300531, 0.50901639, 0.95494943,\n",
              "        0.00871527, 0.71349096, 0.15389491, 0.20240396, 0.71295623,\n",
              "        0.08416778, 0.36764866, 0.4441819 , 0.53238772, 0.8325853 ,\n",
              "        0.49646404, 0.42241915, 0.76408558],\n",
              "       [0.39277614, 0.95607928, 0.59422517, 0.90127016, 0.15877517,\n",
              "        0.61479654, 0.1312027 , 0.39327683, 0.67973786, 0.76378802,\n",
              "        0.6829011 , 0.37916845, 0.23642956, 0.39511709, 0.40478008,\n",
              "        0.46683908, 0.81981797, 0.70955262, 0.03683386, 0.95595552,\n",
              "        0.20875894, 0.03569923, 0.67802348, 0.72743017, 0.54912748,\n",
              "        0.24267335, 0.62006487, 0.56553491],\n",
              "       [0.81621064, 0.46637799, 0.238834  , 0.46645769, 0.58556192,\n",
              "        0.46776437, 0.51109497, 0.23823872, 0.34187811, 0.5851049 ,\n",
              "        0.40796261, 0.44055087, 0.77388669, 0.21899628, 0.32919425,\n",
              "        0.07431223, 0.24822356, 0.27158064, 0.33012085, 0.38024644,\n",
              "        0.14440894, 0.95764437, 0.13199859, 0.36175361, 0.86332015,\n",
              "        0.67426417, 0.45673785, 0.89947184],\n",
              "       [0.55731139, 0.93041724, 0.98405588, 0.97045736, 0.48184262,\n",
              "        0.14940435, 0.50083927, 0.387888  , 0.01617275, 0.09193493,\n",
              "        0.0354179 , 0.28662185, 0.49864093, 0.40741673, 0.39539607,\n",
              "        0.24650831, 0.63291738, 0.01149154, 0.57561426, 0.66363515,\n",
              "        0.49516129, 0.02193591, 0.13498928, 0.78692634, 0.45374232,\n",
              "        0.27471173, 0.13723836, 0.17292462],\n",
              "       [0.67886838, 0.36634614, 0.91994058, 0.68035478, 0.48762756,\n",
              "        0.2815822 , 0.00357239, 0.99286258, 0.26239732, 0.71427737,\n",
              "        0.97819196, 0.35980552, 0.98329871, 0.58153412, 0.72617455,\n",
              "        0.04530105, 0.03644941, 0.57681219, 0.4874463 , 0.20270035,\n",
              "        0.82368399, 0.26587731, 0.1951186 , 0.96365212, 0.34913511,\n",
              "        0.54776789, 0.20423162, 0.59901663],\n",
              "       [0.84556618, 0.83049571, 0.51213194, 0.32786748, 0.46709428,\n",
              "        0.31962694, 0.95399774, 0.76910142, 0.68216497, 0.85889045,\n",
              "        0.95200992, 0.68273474, 0.84155279, 0.268168  , 0.58811376,\n",
              "        0.84193911, 0.17020091, 0.81470511, 0.58113369, 0.55407806,\n",
              "        0.26933605, 0.26401734, 0.93362839, 0.65638436, 0.02341353,\n",
              "        0.38846819, 0.6317186 , 0.71067302],\n",
              "       [0.35755438, 0.7582834 , 0.57909874, 0.41990843, 0.25634725,\n",
              "        0.6808134 , 0.99407198, 0.97865414, 0.55652534, 0.45360362,\n",
              "        0.1276678 , 0.81085315, 0.32125315, 0.80804556, 0.60118175,\n",
              "        0.31693366, 0.54049099, 0.78599239, 0.82904254, 0.73487235,\n",
              "        0.17234274, 0.51756156, 0.53407283, 0.03414904, 0.45543992,\n",
              "        0.00484796, 0.33548409, 0.32441237],\n",
              "       [0.62156033, 0.3364216 , 0.93573225, 0.57498944, 0.29912465,\n",
              "        0.7951667 , 0.89130991, 0.18987333, 0.70736138, 0.70685136,\n",
              "        0.57502004, 0.23803207, 0.13544631, 0.73185712, 0.95170315,\n",
              "        0.24058046, 0.12514861, 0.33212869, 0.66335991, 0.98279728,\n",
              "        0.39653946, 0.51229749, 0.0017588 , 0.41748712, 0.19935317,\n",
              "        0.180024  , 0.9100601 , 0.8026707 ],\n",
              "       [0.48155343, 0.55539266, 0.72364434, 0.13959112, 0.99886979,\n",
              "        0.72774545, 0.16234552, 0.26015127, 0.39704562, 0.84582575,\n",
              "        0.36175454, 0.15449829, 0.0266472 , 0.83739106, 0.59482793,\n",
              "        0.93810865, 0.68229436, 0.93720779, 0.84504128, 0.47154233,\n",
              "        0.00210387, 0.58567006, 0.00346258, 0.55774519, 0.62892407,\n",
              "        0.91994116, 0.47155077, 0.52970466],\n",
              "       [0.80585255, 0.49336107, 0.98337106, 0.99676658, 0.33869998,\n",
              "        0.33078604, 0.34932951, 0.7204608 , 0.78005039, 0.8085295 ,\n",
              "        0.39633067, 0.31957012, 0.88461911, 0.1937973 , 0.66256214,\n",
              "        0.39868438, 0.35196714, 0.37444592, 0.9490226 , 0.13335597,\n",
              "        0.91885948, 0.24289291, 0.20932087, 0.51112971, 0.40839206,\n",
              "        0.14151221, 0.0663215 , 0.99296322],\n",
              "       [0.32640489, 0.97390646, 0.18341379, 0.64632294, 0.43944425,\n",
              "        0.09658054, 0.42093561, 0.73425445, 0.28804637, 0.38863628,\n",
              "        0.78162008, 0.87789587, 0.90330537, 0.77448997, 0.26312829,\n",
              "        0.46107947, 0.52449571, 0.51643663, 0.67971366, 0.10975389,\n",
              "        0.12244041, 0.87572846, 0.75270716, 0.60316764, 0.93243864,\n",
              "        0.36875675, 0.92130033, 0.31762508],\n",
              "       [0.94815277, 0.57022703, 0.71842338, 0.37194228, 0.7471295 ,\n",
              "        0.54049662, 0.38337337, 0.99642682, 0.10700055, 0.06397627,\n",
              "        0.42843744, 0.63381847, 0.46914085, 0.36187215, 0.61013617,\n",
              "        0.4067599 , 0.75148379, 0.34407473, 0.38795724, 0.3222347 ,\n",
              "        0.47279193, 0.44233142, 0.55635828, 0.76772147, 0.20182009,\n",
              "        0.91722134, 0.34938697, 0.76694889],\n",
              "       [0.95899325, 0.63068417, 0.49037266, 0.24154056, 0.94631054,\n",
              "        0.94819617, 0.49869352, 0.23282851, 0.36751159, 0.13782301,\n",
              "        0.04954917, 0.73857674, 0.60191115, 0.31042453, 0.38019259,\n",
              "        0.30022316, 0.52504272, 0.77367535, 0.95299933, 0.50611605,\n",
              "        0.34508704, 0.96407117, 0.36614422, 0.51560578, 0.56311325,\n",
              "        0.55367736, 0.97693227, 0.26507912]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4.3 합성곱 계층 구현하기"
      ],
      "metadata": {
        "id": "YrRERdCr06tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from common.util import im2col\n",
        "\n",
        "x1 = np.random.rand(1, 3, 7, 7)  # (데이터 수, 채널 수, 높이, 너비)\n",
        "col1 = im2col(x1, 5, 5, stride = 1, pad = 0)\n",
        "print(col1.shape)  # (9, 75)\n",
        "\n",
        "x2 = np.random.rand(10, 3, 7, 7)  # 데이터 10개\n",
        "col2 = im2col(x2, 5, 5, stride = 1, pad = 0)\n",
        "print(col2.shape)  # (90, 75)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyd_vxqi1W_P",
        "outputId": "e45bb273-1cb5-4567-9e0f-3994527a1a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9, 75)\n",
            "(90, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Convolution:\n",
        "    def __init__(self, W, b, stride = 1, pad = 0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
        "        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T  # 필터 전개\n",
        "        out = np.dot(col, col_W) + self.b\n",
        "\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "pxK9l9nV1cCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4.4 풀링 계층 구현하기"
      ],
      "metadata": {
        "id": "IEF6lKpk1iId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride = 1, pad = 0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        # ❶ 전개\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        # ❷ 최댓값\n",
        "        out = np.max(col, axis = 1)\n",
        "\n",
        "        # ❸ 성형\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "gPcjzy9G1l-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.5 CNN 구현하기"
      ],
      "metadata": {
        "id": "14Hzmy2J1rHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***common/layers.py***\n",
        "\n",
        "합성곱 계층과 풀링 계층의 완벽한 구현입니다."
      ],
      "metadata": {
        "id": "fhYjPPeIoEXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Convolution:\n",
        "    def __init__(self, W, b, stride=1, pad=0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        # 중간 데이터（backward 시 사용）\n",
        "        self.x = None\n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "\n",
        "        # 가중치와 편향 매개변수의 기울기\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
        "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T\n",
        "\n",
        "        out = np.dot(col, col_W) + self.b\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.col = col\n",
        "        self.col_W = col_W\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        self.dW = np.dot(self.col.T, dout)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "        dcol = np.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "        return dx\n",
        "\n",
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride=2, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        arg_max = np.argmax(col, axis=1)\n",
        "        out = np.max(col, axis=1)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.arg_max = arg_max\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = dout.transpose(0, 2, 3, 1)\n",
        "\n",
        "        pool_size = self.pool_h * self.pool_w\n",
        "        dmax = np.zeros((dout.size, pool_size))\n",
        "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "        dmax = dmax.reshape(dout.shape + (pool_size,))\n",
        "\n",
        "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "1EpiUSK7n8Hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***ch07/simple_convnet.py***"
      ],
      "metadata": {
        "id": "RRwdkFqM1u10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import pickle\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "#from common.layers import *\n",
        "#from common.gradient import numerical_gradient\n",
        "\n",
        "class SimpleConvNet:\n",
        "    def __init__(self, input_dim=(1, 28, 28),\n",
        "                conv_param = {'filter_num':30, 'filter_size':5,\n",
        "                              'pad':0, 'stride':1},\n",
        "                hidden_size = 100, output_size = 10, weight_init_std = 0.01):\n",
        "        filter_num = conv_param['filter_num']\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / \\\n",
        "                            filter_stride + 1\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) *\n",
        "                               (conv_output_size/2))\n",
        "\n",
        "\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "                            np.random.randn(filter_num, input_dim[0],\n",
        "                                            filter_size, filter_size)\n",
        "        self.params['b1'] = np.zeros(filter_num)\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "                            np.random.randn(pool_output_size,\n",
        "                                            hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        self.params['W3'] = weight_init_std * \\\n",
        "                            np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'],\n",
        "                                           self.params['b1'],\n",
        "                                           conv_param['stride'],\n",
        "                                           conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h = 2, pool_w = 2, stride = 2)\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'],\n",
        "                                        self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'],\n",
        "                                        self.params['b3'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        # 순전파\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # 역전파\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'] = self.layers['Conv1'].dW\n",
        "        grads['b1'] = self.layers['Conv1'].db\n",
        "        grads['W2'] = self.layers['Affine1'].dW\n",
        "        grads['b2'] = self.layers['Affine1'].db\n",
        "        grads['W3'] = self.layers['Affine2'].dW\n",
        "        grads['b3'] = self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        acc = 0.0\n",
        "\n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt)\n",
        "\n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다（수치미분）.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        loss_w = lambda w: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in (1, 2, 3):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
        "            self.layers[key].W = self.params['W' + str(i+1)]\n",
        "            self.layers[key].b = self.params['b' + str(i+1)]"
      ],
      "metadata": {
        "id": "mNPnfPfo1yHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***ch07/train_convnet.py***"
      ],
      "metadata": {
        "id": "W1WW54JK2d73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#from dataset.mnist import load_mnist\n",
        "#from simple_convnet import SimpleConvNet\n",
        "#from common.trainer import Trainer\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "# 훈련 시간이 오래 걸려서 경우 데이터를 5000개로 제한\n",
        "x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28),\n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# 매개변수 보존\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "udajcz5u2hhV",
        "outputId": "baecdc1f-f49a-4e23-cc61-d0f9b0b83b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Creating pickle file ...\n",
            "Done!\n",
            "train loss:2.2991055380655734\n",
            "=== epoch:1, train acc:0.107, test acc:0.115 ===\n",
            "train loss:2.297878058954817\n",
            "train loss:2.29402164618933\n",
            "train loss:2.2864312291319244\n",
            "train loss:2.2753498382552606\n",
            "train loss:2.263665958905436\n",
            "train loss:2.263563539749392\n",
            "train loss:2.2393884139725326\n",
            "train loss:2.231542583734738\n",
            "train loss:2.207658623617507\n",
            "train loss:2.1686479016642473\n",
            "train loss:2.143535889274032\n",
            "train loss:2.0904984845473873\n",
            "train loss:2.0452748235865643\n",
            "train loss:1.924054818786942\n",
            "train loss:1.8972895580781985\n",
            "train loss:1.8783683215036406\n",
            "train loss:1.8380117191533358\n",
            "train loss:1.7990345150018467\n",
            "train loss:1.5707569442337224\n",
            "train loss:1.5537811918848292\n",
            "train loss:1.52023636820388\n",
            "train loss:1.4462037418266676\n",
            "train loss:1.2608108632580766\n",
            "train loss:1.2442602689317133\n",
            "train loss:1.1270382289666834\n",
            "train loss:1.0912709848682325\n",
            "train loss:1.080001145981768\n",
            "train loss:0.9318218368501877\n",
            "train loss:0.9548910233725458\n",
            "train loss:0.8058195329542894\n",
            "train loss:0.7255938920306916\n",
            "train loss:0.8167192243718628\n",
            "train loss:0.6824407950888671\n",
            "train loss:0.6957830691034584\n",
            "train loss:0.6211168599051731\n",
            "train loss:0.6608339539511056\n",
            "train loss:0.7162408857168694\n",
            "train loss:0.5256454290401937\n",
            "train loss:0.5969361813019096\n",
            "train loss:0.6040364460097429\n",
            "train loss:0.542103610262487\n",
            "train loss:0.626461545698099\n",
            "train loss:0.475929309681033\n",
            "train loss:0.7473463126196842\n",
            "train loss:0.6686840795023181\n",
            "train loss:0.40572471065780524\n",
            "train loss:0.4440800378609766\n",
            "train loss:0.49846884127001145\n",
            "train loss:0.40189471691342576\n",
            "train loss:0.4089065379060597\n",
            "=== epoch:2, train acc:0.821, test acc:0.801 ===\n",
            "train loss:0.3689975308075216\n",
            "train loss:0.520729691073205\n",
            "train loss:0.6743004155135576\n",
            "train loss:0.5852893927790709\n",
            "train loss:0.851317077976878\n",
            "train loss:0.3634370953793647\n",
            "train loss:0.3436474875613697\n",
            "train loss:0.4212975453125645\n",
            "train loss:0.4396967579710522\n",
            "train loss:0.36269349630977954\n",
            "train loss:0.47491645392299586\n",
            "train loss:0.3467245725240084\n",
            "train loss:0.4953146077068136\n",
            "train loss:0.42543898818485204\n",
            "train loss:0.38762859867986504\n",
            "train loss:0.4456749574083774\n",
            "train loss:0.30896055096409986\n",
            "train loss:0.5149202119761136\n",
            "train loss:0.4061289060583216\n",
            "train loss:0.3091806315117332\n",
            "train loss:0.4541041255658246\n",
            "train loss:0.5689781213527049\n",
            "train loss:0.43073550856708764\n",
            "train loss:0.35478134048465754\n",
            "train loss:0.3493411777268486\n",
            "train loss:0.45557989227921775\n",
            "train loss:0.30490051612945096\n",
            "train loss:0.33326986671849995\n",
            "train loss:0.4765565843801823\n",
            "train loss:0.5165878363699241\n",
            "train loss:0.16860136847172305\n",
            "train loss:0.20179501595546803\n",
            "train loss:0.35296331127387215\n",
            "train loss:0.2547579861987805\n",
            "train loss:0.3297001134136922\n",
            "train loss:0.47150885584427926\n",
            "train loss:0.3086866556426748\n",
            "train loss:0.4108111815193346\n",
            "train loss:0.3459280234056151\n",
            "train loss:0.24059628454841028\n",
            "train loss:0.2738031091922999\n",
            "train loss:0.46164435676208454\n",
            "train loss:0.3920779018364322\n",
            "train loss:0.49103660589564607\n",
            "train loss:0.2822131647755194\n",
            "train loss:0.3552619998808773\n",
            "train loss:0.3664405848538278\n",
            "train loss:0.3849499642335528\n",
            "train loss:0.3334802636412526\n",
            "train loss:0.38119824649869616\n",
            "=== epoch:3, train acc:0.879, test acc:0.867 ===\n",
            "train loss:0.3661309554504577\n",
            "train loss:0.28463785708987777\n",
            "train loss:0.23763037865877737\n",
            "train loss:0.38232837495656674\n",
            "train loss:0.26543502058390794\n",
            "train loss:0.4053552678433544\n",
            "train loss:0.1881887017275278\n",
            "train loss:0.4618877599193937\n",
            "train loss:0.4225809665181572\n",
            "train loss:0.31330502860222653\n",
            "train loss:0.3062313601362977\n",
            "train loss:0.2582500882005321\n",
            "train loss:0.4249413851619757\n",
            "train loss:0.39239289386277165\n",
            "train loss:0.32877311190091\n",
            "train loss:0.27287184857644786\n",
            "train loss:0.36301202566613094\n",
            "train loss:0.24140017116548995\n",
            "train loss:0.23512642735747658\n",
            "train loss:0.3173150985290317\n",
            "train loss:0.3165211453383952\n",
            "train loss:0.35136118401589433\n",
            "train loss:0.33906879852257515\n",
            "train loss:0.23147396725092315\n",
            "train loss:0.2769264755752049\n",
            "train loss:0.4182055020607997\n",
            "train loss:0.31337330095918015\n",
            "train loss:0.26481115990915166\n",
            "train loss:0.2642246392166092\n",
            "train loss:0.43394170397165227\n",
            "train loss:0.1623224245649741\n",
            "train loss:0.32948760934595955\n",
            "train loss:0.13534061882659162\n",
            "train loss:0.3223686523023124\n",
            "train loss:0.22274586314665837\n",
            "train loss:0.28736447742909765\n",
            "train loss:0.2725268701034353\n",
            "train loss:0.273593426924366\n",
            "train loss:0.29824377877474084\n",
            "train loss:0.3302524049821821\n",
            "train loss:0.25798472627645147\n",
            "train loss:0.27916001355210557\n",
            "train loss:0.22522918028268243\n",
            "train loss:0.16928823378063104\n",
            "train loss:0.367826333388183\n",
            "train loss:0.1581694238504955\n",
            "train loss:0.16030481158211138\n",
            "train loss:0.2715841946714047\n",
            "train loss:0.23120798236353263\n",
            "train loss:0.36382237631518843\n",
            "=== epoch:4, train acc:0.913, test acc:0.885 ===\n",
            "train loss:0.23018186611764102\n",
            "train loss:0.3333853078123663\n",
            "train loss:0.2774318272871771\n",
            "train loss:0.3467779854856941\n",
            "train loss:0.27748280568725775\n",
            "train loss:0.19847450591570304\n",
            "train loss:0.30367668954402927\n",
            "train loss:0.24662735469322553\n",
            "train loss:0.1720932208938825\n",
            "train loss:0.26485154065986743\n",
            "train loss:0.22732048401164195\n",
            "train loss:0.22973598476476476\n",
            "train loss:0.16743589644224532\n",
            "train loss:0.24085781999451622\n",
            "train loss:0.29566108383856354\n",
            "train loss:0.259894033724812\n",
            "train loss:0.2850621448189951\n",
            "train loss:0.14475964081284426\n",
            "train loss:0.2820200556751469\n",
            "train loss:0.28945522959021924\n",
            "train loss:0.18625410739988588\n",
            "train loss:0.1277478956451984\n",
            "train loss:0.21581354685746454\n",
            "train loss:0.36973375233013894\n",
            "train loss:0.1497086138138932\n",
            "train loss:0.34818046224612204\n",
            "train loss:0.15984844531316336\n",
            "train loss:0.39564630136412\n",
            "train loss:0.32009506427391005\n",
            "train loss:0.22701924793858744\n",
            "train loss:0.24936194117557098\n",
            "train loss:0.22539881806068648\n",
            "train loss:0.1569029112976171\n",
            "train loss:0.19812153199561586\n",
            "train loss:0.20734448043084966\n",
            "train loss:0.27880611988516696\n",
            "train loss:0.2807120773190772\n",
            "train loss:0.18532057401582114\n",
            "train loss:0.17174998254697635\n",
            "train loss:0.19242510945256455\n",
            "train loss:0.21317467489875397\n",
            "train loss:0.18868142539493088\n",
            "train loss:0.23134534112131605\n",
            "train loss:0.2864534051288156\n",
            "train loss:0.30618984315668046\n",
            "train loss:0.13247380325016034\n",
            "train loss:0.24886503639931445\n",
            "train loss:0.32064934837843895\n",
            "train loss:0.17475761113217833\n",
            "train loss:0.21015191521478285\n",
            "=== epoch:5, train acc:0.929, test acc:0.914 ===\n",
            "train loss:0.1651948481175486\n",
            "train loss:0.3577708747715009\n",
            "train loss:0.22513988568093798\n",
            "train loss:0.2351081899850023\n",
            "train loss:0.1356508498641083\n",
            "train loss:0.30242265170766475\n",
            "train loss:0.384744088223017\n",
            "train loss:0.17744921629856544\n",
            "train loss:0.20766051090824716\n",
            "train loss:0.17044575525796765\n",
            "train loss:0.16624564379942636\n",
            "train loss:0.14292476057142717\n",
            "train loss:0.10724364696616422\n",
            "train loss:0.16422918681726575\n",
            "train loss:0.2904205079524687\n",
            "train loss:0.2810246686228643\n",
            "train loss:0.16207395273661862\n",
            "train loss:0.14393768590921802\n",
            "train loss:0.351217581268573\n",
            "train loss:0.21304970626689596\n",
            "train loss:0.18575396138296432\n",
            "train loss:0.1974357957188335\n",
            "train loss:0.29079859027529287\n",
            "train loss:0.2359903392073723\n",
            "train loss:0.11496274134502632\n",
            "train loss:0.1175307371029294\n",
            "train loss:0.20687371772139443\n",
            "train loss:0.21126739579659\n",
            "train loss:0.22247886730265168\n",
            "train loss:0.2487886300422607\n",
            "train loss:0.21697544715740072\n",
            "train loss:0.22586034595729493\n",
            "train loss:0.2930007856888718\n",
            "train loss:0.1798532355169327\n",
            "train loss:0.20109857933171932\n",
            "train loss:0.16306718701460085\n",
            "train loss:0.18093830996111968\n",
            "train loss:0.22364630270090674\n",
            "train loss:0.09966524013385726\n",
            "train loss:0.2758972897445356\n",
            "train loss:0.18465510502469978\n",
            "train loss:0.190445891531438\n",
            "train loss:0.16132863858189403\n",
            "train loss:0.1465635757395754\n",
            "train loss:0.22865056763185806\n",
            "train loss:0.23674453354174915\n",
            "train loss:0.1856037195942315\n",
            "train loss:0.2774282483381735\n",
            "train loss:0.1630399308591306\n",
            "train loss:0.12833004389414296\n",
            "=== epoch:6, train acc:0.937, test acc:0.917 ===\n",
            "train loss:0.20101601914049755\n",
            "train loss:0.1804750167154137\n",
            "train loss:0.2090609971244545\n",
            "train loss:0.19973998212754837\n",
            "train loss:0.11716052672223291\n",
            "train loss:0.12788372483364682\n",
            "train loss:0.13746253783025736\n",
            "train loss:0.15078104457673813\n",
            "train loss:0.08061145163476131\n",
            "train loss:0.23236420987602954\n",
            "train loss:0.13370779203107291\n",
            "train loss:0.07887043653905795\n",
            "train loss:0.14446889859091663\n",
            "train loss:0.19800952374821246\n",
            "train loss:0.16137534202348924\n",
            "train loss:0.19819455257194057\n",
            "train loss:0.2097033868532282\n",
            "train loss:0.06944970007926467\n",
            "train loss:0.17008881845908583\n",
            "train loss:0.14639155763065634\n",
            "train loss:0.31863755102834684\n",
            "train loss:0.10195834196039547\n",
            "train loss:0.1269825453177937\n",
            "train loss:0.11045567660166325\n",
            "train loss:0.16477245262606866\n",
            "train loss:0.11481736912503689\n",
            "train loss:0.17999094994605286\n",
            "train loss:0.1111705164184603\n",
            "train loss:0.14882693538083808\n",
            "train loss:0.12788376017195197\n",
            "train loss:0.08134307174245954\n",
            "train loss:0.12219314924333613\n",
            "train loss:0.14354222433761257\n",
            "train loss:0.17976808622278428\n",
            "train loss:0.13392259326270736\n",
            "train loss:0.10996878310374057\n",
            "train loss:0.06347001758752953\n",
            "train loss:0.14875625141488594\n",
            "train loss:0.19554478565038505\n",
            "train loss:0.1644698792917454\n",
            "train loss:0.1478061607246217\n",
            "train loss:0.19289804372514704\n",
            "train loss:0.12806323589340024\n",
            "train loss:0.11945476543525302\n",
            "train loss:0.2980783353920255\n",
            "train loss:0.20071553546720666\n",
            "train loss:0.19959128441045762\n",
            "train loss:0.06078387895970993\n",
            "train loss:0.1681825744726363\n",
            "train loss:0.14341475556468444\n",
            "=== epoch:7, train acc:0.942, test acc:0.93 ===\n",
            "train loss:0.08243240652084255\n",
            "train loss:0.08840450867697096\n",
            "train loss:0.0530102561395403\n",
            "train loss:0.16484544955944905\n",
            "train loss:0.11851388494313703\n",
            "train loss:0.12027624923710299\n",
            "train loss:0.04396303830515186\n",
            "train loss:0.10486169365692607\n",
            "train loss:0.1007267832826709\n",
            "train loss:0.1665986365417054\n",
            "train loss:0.38424914639618435\n",
            "train loss:0.1370078936063663\n",
            "train loss:0.09981459990945594\n",
            "train loss:0.1207106808879181\n",
            "train loss:0.12163129754744688\n",
            "train loss:0.1438767672417969\n",
            "train loss:0.16880812149472327\n",
            "train loss:0.11106802521141484\n",
            "train loss:0.1095152026071438\n",
            "train loss:0.13023219348213158\n",
            "train loss:0.11475613014044572\n",
            "train loss:0.1295055025178903\n",
            "train loss:0.081264063614611\n",
            "train loss:0.2199901846633877\n",
            "train loss:0.19986600402017288\n",
            "train loss:0.08723421813403862\n",
            "train loss:0.20460613351828216\n",
            "train loss:0.20786244002033805\n",
            "train loss:0.11047501023513301\n",
            "train loss:0.1575039823342389\n",
            "train loss:0.13711922821208458\n",
            "train loss:0.1588088970407313\n",
            "train loss:0.11484750186057952\n",
            "train loss:0.19866913989762294\n",
            "train loss:0.1487274089291121\n",
            "train loss:0.19582097349381533\n",
            "train loss:0.09825490043644815\n",
            "train loss:0.18403642261549735\n",
            "train loss:0.13382478752018748\n",
            "train loss:0.18422471334041518\n",
            "train loss:0.10819899281243091\n",
            "train loss:0.1318305152105047\n",
            "train loss:0.18788871985455566\n",
            "train loss:0.0639176281682063\n",
            "train loss:0.14091885122377082\n",
            "train loss:0.06361930583559922\n",
            "train loss:0.16439763944274932\n",
            "train loss:0.146812553139433\n",
            "train loss:0.10985409428481824\n",
            "train loss:0.11331102541755614\n",
            "=== epoch:8, train acc:0.95, test acc:0.932 ===\n",
            "train loss:0.16644917246313576\n",
            "train loss:0.09661319822324162\n",
            "train loss:0.1427815195309003\n",
            "train loss:0.1258717802049202\n",
            "train loss:0.08194900337473253\n",
            "train loss:0.0579711407340697\n",
            "train loss:0.08501499095614391\n",
            "train loss:0.09299851010736379\n",
            "train loss:0.08516372847797163\n",
            "train loss:0.1505119379715136\n",
            "train loss:0.09259864072706657\n",
            "train loss:0.14987291186191712\n",
            "train loss:0.07724121940235498\n",
            "train loss:0.1271672901463755\n",
            "train loss:0.12035984696779335\n",
            "train loss:0.06921878344915727\n",
            "train loss:0.11099829891316591\n",
            "train loss:0.1318028861463059\n",
            "train loss:0.14977013031112826\n",
            "train loss:0.08419794530168367\n",
            "train loss:0.07641577780992369\n",
            "train loss:0.062094138934812186\n",
            "train loss:0.04079988297595407\n",
            "train loss:0.08954314681040725\n",
            "train loss:0.21497569677245934\n",
            "train loss:0.12993749787853584\n",
            "train loss:0.023497250237786226\n",
            "train loss:0.060618960327477246\n",
            "train loss:0.13965261321962777\n",
            "train loss:0.1032616649055379\n",
            "train loss:0.14677460147074844\n",
            "train loss:0.17219653441879587\n",
            "train loss:0.10681966640092125\n",
            "train loss:0.12717403056614585\n",
            "train loss:0.07279418168829487\n",
            "train loss:0.07666229663530974\n",
            "train loss:0.07413196293989337\n",
            "train loss:0.04492595555928572\n",
            "train loss:0.058456605655266294\n",
            "train loss:0.06787492101737427\n",
            "train loss:0.1942695212779588\n",
            "train loss:0.118729286700101\n",
            "train loss:0.16527861130115742\n",
            "train loss:0.10833501613741121\n",
            "train loss:0.11958862174973667\n",
            "train loss:0.058222260954320496\n",
            "train loss:0.08578536255281487\n",
            "train loss:0.10854641028336516\n",
            "train loss:0.04051744974742738\n",
            "train loss:0.12188298224023099\n",
            "=== epoch:9, train acc:0.958, test acc:0.941 ===\n",
            "train loss:0.12084630694424452\n",
            "train loss:0.08712567356715356\n",
            "train loss:0.09148543020428755\n",
            "train loss:0.07217420204567841\n",
            "train loss:0.07327686945853612\n",
            "train loss:0.07917696088140411\n",
            "train loss:0.04263906039942192\n",
            "train loss:0.15595676106818251\n",
            "train loss:0.10143835329215907\n",
            "train loss:0.07866143978804283\n",
            "train loss:0.04470284726999898\n",
            "train loss:0.10305715551622129\n",
            "train loss:0.05210308620359221\n",
            "train loss:0.07056241540163873\n",
            "train loss:0.06019684937564903\n",
            "train loss:0.10231431243670906\n",
            "train loss:0.1565906816080886\n",
            "train loss:0.05250356057551117\n",
            "train loss:0.05988189595487608\n",
            "train loss:0.09612080961063711\n",
            "train loss:0.16436706589677233\n",
            "train loss:0.12210951397584548\n",
            "train loss:0.11460012313909523\n",
            "train loss:0.18103653204451178\n",
            "train loss:0.11009046118817087\n",
            "train loss:0.12362625089811902\n",
            "train loss:0.06615429121126798\n",
            "train loss:0.07568769601754019\n",
            "train loss:0.10335925611890608\n",
            "train loss:0.08352879972914545\n",
            "train loss:0.056929361351708944\n",
            "train loss:0.09608762393411013\n",
            "train loss:0.0494618787770755\n",
            "train loss:0.05015986761777794\n",
            "train loss:0.04811458459812701\n",
            "train loss:0.12986829507794184\n",
            "train loss:0.10569210901308919\n",
            "train loss:0.060178506311103164\n",
            "train loss:0.08047584159436491\n",
            "train loss:0.0657525465722187\n",
            "train loss:0.06726164162553318\n",
            "train loss:0.08341330164646074\n",
            "train loss:0.1734943658141958\n",
            "train loss:0.05239373843081804\n",
            "train loss:0.12265915883992162\n",
            "train loss:0.1198099481859937\n",
            "train loss:0.14651324692972761\n",
            "train loss:0.09613934458487483\n",
            "train loss:0.054591731669971805\n",
            "train loss:0.07896115712778957\n",
            "=== epoch:10, train acc:0.967, test acc:0.942 ===\n",
            "train loss:0.08092869398424578\n",
            "train loss:0.07568202295629789\n",
            "train loss:0.07955623920180802\n",
            "train loss:0.07481021354083164\n",
            "train loss:0.14437024114940758\n",
            "train loss:0.16163626176307208\n",
            "train loss:0.1743429568309622\n",
            "train loss:0.1183700898194127\n",
            "train loss:0.10374721069295888\n",
            "train loss:0.08374517315203614\n",
            "train loss:0.0866882262940802\n",
            "train loss:0.037256538836774454\n",
            "train loss:0.06524024892416848\n",
            "train loss:0.18494589112853121\n",
            "train loss:0.15741279796922\n",
            "train loss:0.09598620265638097\n",
            "train loss:0.13504964052866078\n",
            "train loss:0.10832043182968619\n",
            "train loss:0.04602524298456974\n",
            "train loss:0.052462952321412765\n",
            "train loss:0.05342041661161176\n",
            "train loss:0.10923576997573113\n",
            "train loss:0.05024459147410434\n",
            "train loss:0.10435712420729301\n",
            "train loss:0.05124696574581079\n",
            "train loss:0.12237602587526623\n",
            "train loss:0.050316252766485006\n",
            "train loss:0.06813983137679828\n",
            "train loss:0.056704148589927984\n",
            "train loss:0.06329217134845012\n",
            "train loss:0.11790705978171331\n",
            "train loss:0.09177991586575722\n",
            "train loss:0.08014551766835985\n",
            "train loss:0.058707683481335454\n",
            "train loss:0.08724362004035198\n",
            "train loss:0.055560365720331545\n",
            "train loss:0.04950043992370478\n",
            "train loss:0.03760292448685491\n",
            "train loss:0.10955397419261902\n",
            "train loss:0.08543581574961676\n",
            "train loss:0.11213650414155313\n",
            "train loss:0.07247808107777326\n",
            "train loss:0.08272625923453242\n",
            "train loss:0.055214014594244895\n",
            "train loss:0.06873376067682398\n",
            "train loss:0.0513170754093374\n",
            "train loss:0.024439872550207995\n",
            "train loss:0.03740711877445682\n",
            "train loss:0.08826433723522267\n",
            "train loss:0.07973388963267034\n",
            "=== epoch:11, train acc:0.973, test acc:0.954 ===\n",
            "train loss:0.04873321172589375\n",
            "train loss:0.0805411850603615\n",
            "train loss:0.16300362515227657\n",
            "train loss:0.1019108685077015\n",
            "train loss:0.0797117976301401\n",
            "train loss:0.06134502005094708\n",
            "train loss:0.07621052192417953\n",
            "train loss:0.13147417726311247\n",
            "train loss:0.16325624338208886\n",
            "train loss:0.05136552121911278\n",
            "train loss:0.12551893935358616\n",
            "train loss:0.05966104742613543\n",
            "train loss:0.0416850380622664\n",
            "train loss:0.07416756722946767\n",
            "train loss:0.09706607349252011\n",
            "train loss:0.04602823787279098\n",
            "train loss:0.10556774334902748\n",
            "train loss:0.05007464351186016\n",
            "train loss:0.11544475625204996\n",
            "train loss:0.05390876101578299\n",
            "train loss:0.021076831656671794\n",
            "train loss:0.06447461214922283\n",
            "train loss:0.08799098163735054\n",
            "train loss:0.031030191681520884\n",
            "train loss:0.09165469798089937\n",
            "train loss:0.06037836766105904\n",
            "train loss:0.10239046821168815\n",
            "train loss:0.07299052255734789\n",
            "train loss:0.09925556798149314\n",
            "train loss:0.03130908192371312\n",
            "train loss:0.08125992697198109\n",
            "train loss:0.18835861215311922\n",
            "train loss:0.08632161530011412\n",
            "train loss:0.03282618085311631\n",
            "train loss:0.05747322776293472\n",
            "train loss:0.04824522479468071\n",
            "train loss:0.05852057583320845\n",
            "train loss:0.11380315626762841\n",
            "train loss:0.08860585938165665\n",
            "train loss:0.0770695008287396\n",
            "train loss:0.043166626055224595\n",
            "train loss:0.08454968022041802\n",
            "train loss:0.0588824313305423\n",
            "train loss:0.04719240028839486\n",
            "train loss:0.05629188056620667\n",
            "train loss:0.0306522373361186\n",
            "train loss:0.04650556856165091\n",
            "train loss:0.02684180154871761\n",
            "train loss:0.13749423536709546\n",
            "train loss:0.05340322876093731\n",
            "=== epoch:12, train acc:0.976, test acc:0.952 ===\n",
            "train loss:0.06522899375771829\n",
            "train loss:0.048415892796517396\n",
            "train loss:0.048252558040702415\n",
            "train loss:0.07735628875621697\n",
            "train loss:0.03576869666287086\n",
            "train loss:0.048060730873786726\n",
            "train loss:0.13458660133325895\n",
            "train loss:0.032005246507895733\n",
            "train loss:0.08348315846978713\n",
            "train loss:0.10824362640197872\n",
            "train loss:0.05631627334539314\n",
            "train loss:0.08056226214513039\n",
            "train loss:0.05722324883517072\n",
            "train loss:0.024275895849976848\n",
            "train loss:0.07355705468717862\n",
            "train loss:0.16306245520901136\n",
            "train loss:0.09856321926689264\n",
            "train loss:0.033819489504698944\n",
            "train loss:0.022928039792457226\n",
            "train loss:0.12462013250727302\n",
            "train loss:0.06637988650137824\n",
            "train loss:0.06624221917418671\n",
            "train loss:0.03129163642172351\n",
            "train loss:0.03155315823594313\n",
            "train loss:0.05964770476357767\n",
            "train loss:0.0442137398427561\n",
            "train loss:0.02837580858270461\n",
            "train loss:0.04630775966306568\n",
            "train loss:0.04014436522155084\n",
            "train loss:0.026059847542730972\n",
            "train loss:0.039670150019187264\n",
            "train loss:0.05990574660228813\n",
            "train loss:0.0590256411863608\n",
            "train loss:0.054841888098823996\n",
            "train loss:0.03053187165989376\n",
            "train loss:0.034227130495505576\n",
            "train loss:0.10184120379952871\n",
            "train loss:0.047523951948982804\n",
            "train loss:0.05163950878799161\n",
            "train loss:0.017934183913533557\n",
            "train loss:0.04564276184291187\n",
            "train loss:0.060005644221846906\n",
            "train loss:0.06136268536292337\n",
            "train loss:0.04562312624489848\n",
            "train loss:0.05644547753128767\n",
            "train loss:0.04954568538547095\n",
            "train loss:0.08291531780943268\n",
            "train loss:0.011913037078093497\n",
            "train loss:0.03981029564822459\n",
            "train loss:0.06922649851413147\n",
            "=== epoch:13, train acc:0.978, test acc:0.947 ===\n",
            "train loss:0.043667344385767144\n",
            "train loss:0.041253827017609906\n",
            "train loss:0.040489254041985154\n",
            "train loss:0.10033622658500752\n",
            "train loss:0.04420655567649726\n",
            "train loss:0.06912311596632065\n",
            "train loss:0.08307234440932207\n",
            "train loss:0.06416809078776804\n",
            "train loss:0.034222747913197245\n",
            "train loss:0.06624676722263705\n",
            "train loss:0.06893215464151958\n",
            "train loss:0.03885465884129639\n",
            "train loss:0.21103422648052794\n",
            "train loss:0.05700182465867184\n",
            "train loss:0.06003434081674932\n",
            "train loss:0.05019074047445152\n",
            "train loss:0.1110416136635971\n",
            "train loss:0.026534383331116213\n",
            "train loss:0.04872847915093465\n",
            "train loss:0.07697371486023435\n",
            "train loss:0.060712619909584056\n",
            "train loss:0.03660277986165484\n",
            "train loss:0.04328175328309013\n",
            "train loss:0.016831999802979173\n",
            "train loss:0.027093726958117716\n",
            "train loss:0.065957415305829\n",
            "train loss:0.03674646946020804\n",
            "train loss:0.0331508584080869\n",
            "train loss:0.04141902937289687\n",
            "train loss:0.03830122841866365\n",
            "train loss:0.0477298527399255\n",
            "train loss:0.053949275504140894\n",
            "train loss:0.04734356948715703\n",
            "train loss:0.020442403316358812\n",
            "train loss:0.047393473887072164\n",
            "train loss:0.040053317387313714\n",
            "train loss:0.03880740173513848\n",
            "train loss:0.09052239608034275\n",
            "train loss:0.030319554858045238\n",
            "train loss:0.05134084972611696\n",
            "train loss:0.03664173398080435\n",
            "train loss:0.03158465549099356\n",
            "train loss:0.06300475852362598\n",
            "train loss:0.0308834624079009\n",
            "train loss:0.05859112653108243\n",
            "train loss:0.0513026722139518\n",
            "train loss:0.06748276736702008\n",
            "train loss:0.04736192165631994\n",
            "train loss:0.03711819842532033\n",
            "train loss:0.026480446044599412\n",
            "=== epoch:14, train acc:0.976, test acc:0.959 ===\n",
            "train loss:0.03816640307826313\n",
            "train loss:0.0453924699668655\n",
            "train loss:0.027095444070916817\n",
            "train loss:0.07276271151097954\n",
            "train loss:0.043930097562848644\n",
            "train loss:0.02516431358138889\n",
            "train loss:0.061328832999242966\n",
            "train loss:0.034526644763623875\n",
            "train loss:0.008511196704273527\n",
            "train loss:0.02231926568392803\n",
            "train loss:0.048676366806813906\n",
            "train loss:0.022563822349655108\n",
            "train loss:0.04221908681836922\n",
            "train loss:0.08893488499086676\n",
            "train loss:0.09887388303884462\n",
            "train loss:0.025148397714894504\n",
            "train loss:0.03558691630270397\n",
            "train loss:0.021587057761353966\n",
            "train loss:0.042349703598069235\n",
            "train loss:0.01562656509338959\n",
            "train loss:0.1582669202590513\n",
            "train loss:0.03896094394445732\n",
            "train loss:0.05874978132898326\n",
            "train loss:0.0690053422370957\n",
            "train loss:0.04854045144762144\n",
            "train loss:0.03587592703240068\n",
            "train loss:0.07235273723254589\n",
            "train loss:0.036450477331509544\n",
            "train loss:0.03737438768414017\n",
            "train loss:0.08809777923228923\n",
            "train loss:0.024141326515022574\n",
            "train loss:0.07667610947839192\n",
            "train loss:0.051988086844511755\n",
            "train loss:0.01856185819605706\n",
            "train loss:0.047660057303711555\n",
            "train loss:0.045965378537401035\n",
            "train loss:0.04280319536025859\n",
            "train loss:0.039586591581329184\n",
            "train loss:0.02157504444948726\n",
            "train loss:0.04812326098513805\n",
            "train loss:0.06585620764208296\n",
            "train loss:0.017841574489155358\n",
            "train loss:0.019131908745362587\n",
            "train loss:0.04323679766238524\n",
            "train loss:0.03780322555127188\n",
            "train loss:0.03141287412506194\n",
            "train loss:0.04123916764439419\n",
            "train loss:0.046257885785954335\n",
            "train loss:0.03596449131568803\n",
            "train loss:0.04748750333670322\n",
            "=== epoch:15, train acc:0.986, test acc:0.964 ===\n",
            "train loss:0.059177317914132666\n",
            "train loss:0.10264013864084555\n",
            "train loss:0.041892316337650656\n",
            "train loss:0.024462386672864943\n",
            "train loss:0.05759270821640892\n",
            "train loss:0.05842562165388298\n",
            "train loss:0.03500809620713072\n",
            "train loss:0.014553948282782947\n",
            "train loss:0.01990726105728066\n",
            "train loss:0.02155326482015218\n",
            "train loss:0.022140916446787614\n",
            "train loss:0.057980378548188466\n",
            "train loss:0.043360592327838995\n",
            "train loss:0.061656931134993834\n",
            "train loss:0.018612943438436058\n",
            "train loss:0.03493225127766985\n",
            "train loss:0.033822845193061626\n",
            "train loss:0.0409657974688957\n",
            "train loss:0.05490470708579675\n",
            "train loss:0.04238159989827064\n",
            "train loss:0.015151303564732176\n",
            "train loss:0.023015670635899727\n",
            "train loss:0.06290354873567292\n",
            "train loss:0.04833358972095791\n",
            "train loss:0.049173668533170824\n",
            "train loss:0.01999455268695275\n",
            "train loss:0.012233457961293095\n",
            "train loss:0.042786331601844536\n",
            "train loss:0.01900280711989747\n",
            "train loss:0.05417246796014173\n",
            "train loss:0.08836356565089903\n",
            "train loss:0.02377982863598492\n",
            "train loss:0.0588295045261479\n",
            "train loss:0.03481140416217966\n",
            "train loss:0.12085836427790769\n",
            "train loss:0.0491692611609573\n",
            "train loss:0.01891742076288893\n",
            "train loss:0.017004647010130616\n",
            "train loss:0.08855498759735735\n",
            "train loss:0.03684896889049079\n",
            "train loss:0.02020666342740332\n",
            "train loss:0.016617175484040313\n",
            "train loss:0.01585487610291266\n",
            "train loss:0.0319538791822329\n",
            "train loss:0.04237113200540923\n",
            "train loss:0.028546177710184112\n",
            "train loss:0.02713323603575636\n",
            "train loss:0.01991549319378519\n",
            "train loss:0.0226967403900276\n",
            "train loss:0.04546704974411982\n",
            "=== epoch:16, train acc:0.981, test acc:0.958 ===\n",
            "train loss:0.024318629206067314\n",
            "train loss:0.011502717580740576\n",
            "train loss:0.02811846478725816\n",
            "train loss:0.03968958235249002\n",
            "train loss:0.039235733533223224\n",
            "train loss:0.031273419030899204\n",
            "train loss:0.0898971691815749\n",
            "train loss:0.013059088502353827\n",
            "train loss:0.01887100386072044\n",
            "train loss:0.02347202721161165\n",
            "train loss:0.046854154858375935\n",
            "train loss:0.02108842237912122\n",
            "train loss:0.026821088089796077\n",
            "train loss:0.014545527906704055\n",
            "train loss:0.011216063383075969\n",
            "train loss:0.018056834478450984\n",
            "train loss:0.02794830291149584\n",
            "train loss:0.0885247077592897\n",
            "train loss:0.049867963110768604\n",
            "train loss:0.018369485493001613\n",
            "train loss:0.02666212250082795\n",
            "train loss:0.0458744679475992\n",
            "train loss:0.04587211372058324\n",
            "train loss:0.019786641993328836\n",
            "train loss:0.05619257975199996\n",
            "train loss:0.0686529037647724\n",
            "train loss:0.010787518900841913\n",
            "train loss:0.0397174737855141\n",
            "train loss:0.05500597380544793\n",
            "train loss:0.04898630971151567\n",
            "train loss:0.020294256323200727\n",
            "train loss:0.04491870070365352\n",
            "train loss:0.07544164909518561\n",
            "train loss:0.029923901017198436\n",
            "train loss:0.031812967544353664\n",
            "train loss:0.01296116562513825\n",
            "train loss:0.012937065552663187\n",
            "train loss:0.02090028549289772\n",
            "train loss:0.06176757375251049\n",
            "train loss:0.0576230055639463\n",
            "train loss:0.09741446562423751\n",
            "train loss:0.00819828838439113\n",
            "train loss:0.034712682838185585\n",
            "train loss:0.05074097547607041\n",
            "train loss:0.01813212038463172\n",
            "train loss:0.04418784223590361\n",
            "train loss:0.025315725240722373\n",
            "train loss:0.04386799046247201\n",
            "train loss:0.020756366017499306\n",
            "train loss:0.03036189660375814\n",
            "=== epoch:17, train acc:0.987, test acc:0.962 ===\n",
            "train loss:0.028292931498504422\n",
            "train loss:0.013615759573891814\n",
            "train loss:0.03728721326678996\n",
            "train loss:0.06241129112034172\n",
            "train loss:0.024501126342980115\n",
            "train loss:0.025254240447470763\n",
            "train loss:0.04174775935756489\n",
            "train loss:0.07130651156402562\n",
            "train loss:0.025445493494352423\n",
            "train loss:0.008799791585563408\n",
            "train loss:0.024561922141924427\n",
            "train loss:0.029734942363010018\n",
            "train loss:0.03985770211571611\n",
            "train loss:0.030250208973970398\n",
            "train loss:0.016559579561037374\n",
            "train loss:0.02267267311364053\n",
            "train loss:0.023657428055369776\n",
            "train loss:0.014814645997032463\n",
            "train loss:0.035441822530779855\n",
            "train loss:0.021513516246183594\n",
            "train loss:0.02515103602686895\n",
            "train loss:0.03415213638106666\n",
            "train loss:0.07040297938838239\n",
            "train loss:0.060813602627582355\n",
            "train loss:0.013908883176427966\n",
            "train loss:0.016033519819987012\n",
            "train loss:0.044279386572566796\n",
            "train loss:0.03206890616173627\n",
            "train loss:0.038384317401961014\n",
            "train loss:0.01114131894865901\n",
            "train loss:0.010298972760767881\n",
            "train loss:0.01986795019721488\n",
            "train loss:0.007838587744602771\n",
            "train loss:0.04846642832670352\n",
            "train loss:0.02177726878537595\n",
            "train loss:0.01764342231113342\n",
            "train loss:0.02247318009290484\n",
            "train loss:0.03333364745530564\n",
            "train loss:0.012567025270922622\n",
            "train loss:0.024735735612008463\n",
            "train loss:0.006782310367389549\n",
            "train loss:0.017534108370335186\n",
            "train loss:0.023063158159455435\n",
            "train loss:0.017514294690189627\n",
            "train loss:0.024669087694310726\n",
            "train loss:0.045864590464999334\n",
            "train loss:0.022350085526222085\n",
            "train loss:0.012205690932852105\n",
            "train loss:0.012348696426268145\n",
            "train loss:0.028701695988747472\n",
            "=== epoch:18, train acc:0.986, test acc:0.955 ===\n",
            "train loss:0.024025192377240226\n",
            "train loss:0.047617379908517644\n",
            "train loss:0.032515416817099185\n",
            "train loss:0.014157108873170019\n",
            "train loss:0.035567514534654614\n",
            "train loss:0.017080734073306788\n",
            "train loss:0.01741009487321292\n",
            "train loss:0.014493742715576208\n",
            "train loss:0.0195705290195271\n",
            "train loss:0.023136880051295286\n",
            "train loss:0.010885906986866306\n",
            "train loss:0.028393121019452395\n",
            "train loss:0.026788683643669476\n",
            "train loss:0.012220539312212543\n",
            "train loss:0.02959585804349774\n",
            "train loss:0.012930287486630419\n",
            "train loss:0.034092778199055825\n",
            "train loss:0.012084457884924881\n",
            "train loss:0.014746672562856698\n",
            "train loss:0.030518506160694486\n",
            "train loss:0.0082754962798501\n",
            "train loss:0.013824353237381525\n",
            "train loss:0.02734484327227834\n",
            "train loss:0.011912100009859244\n",
            "train loss:0.029023705823135813\n",
            "train loss:0.012616080424263154\n",
            "train loss:0.013409584598710373\n",
            "train loss:0.02007518308121034\n",
            "train loss:0.022685137390488032\n",
            "train loss:0.012159723082293723\n",
            "train loss:0.009340782675885896\n",
            "train loss:0.03928272046535674\n",
            "train loss:0.02375256326283225\n",
            "train loss:0.02120623822775882\n",
            "train loss:0.007793473112656983\n",
            "train loss:0.013742867295209245\n",
            "train loss:0.037508869800010086\n",
            "train loss:0.014096477611548903\n",
            "train loss:0.015354571718407926\n",
            "train loss:0.008515730253749115\n",
            "train loss:0.014411647215328777\n",
            "train loss:0.018341288641237255\n",
            "train loss:0.01849677738827219\n",
            "train loss:0.016504008085091462\n",
            "train loss:0.03181546550252957\n",
            "train loss:0.011161283332265747\n",
            "train loss:0.009646149182036233\n",
            "train loss:0.014935583818499343\n",
            "train loss:0.02520894912071132\n",
            "train loss:0.009753679578529179\n",
            "=== epoch:19, train acc:0.993, test acc:0.962 ===\n",
            "train loss:0.006372506809294627\n",
            "train loss:0.012721409159869074\n",
            "train loss:0.03297736991051706\n",
            "train loss:0.0088309516069604\n",
            "train loss:0.018492085555642867\n",
            "train loss:0.013077429873970538\n",
            "train loss:0.021180303955299726\n",
            "train loss:0.017386635202330115\n",
            "train loss:0.021091692976967315\n",
            "train loss:0.006938702261045825\n",
            "train loss:0.036560969580643914\n",
            "train loss:0.014224227951392679\n",
            "train loss:0.008958971746245317\n",
            "train loss:0.01432079709429051\n",
            "train loss:0.007535188319972218\n",
            "train loss:0.007953487412419044\n",
            "train loss:0.013519655601119684\n",
            "train loss:0.011513970417589054\n",
            "train loss:0.019821125633968106\n",
            "train loss:0.01696553677062505\n",
            "train loss:0.021094449803827686\n",
            "train loss:0.012055076247043425\n",
            "train loss:0.00899608976640154\n",
            "train loss:0.014247048258529784\n",
            "train loss:0.01191036679814596\n",
            "train loss:0.007983450349753849\n",
            "train loss:0.022493623905433892\n",
            "train loss:0.035126253273055276\n",
            "train loss:0.007164913344857998\n",
            "train loss:0.040625763740488506\n",
            "train loss:0.0286921641437498\n",
            "train loss:0.007874671057052675\n",
            "train loss:0.014038169890515035\n",
            "train loss:0.012355962872264365\n",
            "train loss:0.016524101522100737\n",
            "train loss:0.024057479165220882\n",
            "train loss:0.045268534385402265\n",
            "train loss:0.025581348025833818\n",
            "train loss:0.011472114604746326\n",
            "train loss:0.02583090399648722\n",
            "train loss:0.013896225025226288\n",
            "train loss:0.009784073171580102\n",
            "train loss:0.007830403573911859\n",
            "train loss:0.018641454274840392\n",
            "train loss:0.01020101468447757\n",
            "train loss:0.01916086939026141\n",
            "train loss:0.01905499251116617\n",
            "train loss:0.02676091468879065\n",
            "train loss:0.014655184890591304\n",
            "train loss:0.024846296203501032\n",
            "=== epoch:20, train acc:0.994, test acc:0.961 ===\n",
            "train loss:0.013426142912567485\n",
            "train loss:0.013076973128091269\n",
            "train loss:0.019254828727926637\n",
            "train loss:0.027319665767324842\n",
            "train loss:0.005412310927932509\n",
            "train loss:0.01343861123921377\n",
            "train loss:0.01261459931174899\n",
            "train loss:0.008508981693535077\n",
            "train loss:0.0076661738207146205\n",
            "train loss:0.017745329929156055\n",
            "train loss:0.011490851381746936\n",
            "train loss:0.008807366542348464\n",
            "train loss:0.006132032544327148\n",
            "train loss:0.0067639515672474025\n",
            "train loss:0.004773372476538181\n",
            "train loss:0.016339553920925426\n",
            "train loss:0.019796740692943092\n",
            "train loss:0.03530983925155346\n",
            "train loss:0.017215429415702914\n",
            "train loss:0.015774150032143542\n",
            "train loss:0.017328458699357407\n",
            "train loss:0.022329587196995643\n",
            "train loss:0.021971878567013006\n",
            "train loss:0.009497275715323989\n",
            "train loss:0.013677680400868981\n",
            "train loss:0.022784479867205016\n",
            "train loss:0.015623351728472236\n",
            "train loss:0.029928631893761982\n",
            "train loss:0.030114426187918517\n",
            "train loss:0.011769524222404525\n",
            "train loss:0.035653522397935365\n",
            "train loss:0.0041732282592131485\n",
            "train loss:0.026053972163350897\n",
            "train loss:0.012058672342180377\n",
            "train loss:0.012433393595475436\n",
            "train loss:0.024775737444312114\n",
            "train loss:0.01409613504985181\n",
            "train loss:0.012483785962760119\n",
            "train loss:0.005698476924165426\n",
            "train loss:0.020061128884303562\n",
            "train loss:0.004310492284390504\n",
            "train loss:0.01660291904116512\n",
            "train loss:0.026162987025722777\n",
            "train loss:0.005940284989413403\n",
            "train loss:0.017758303995997957\n",
            "train loss:0.004727158472365791\n",
            "train loss:0.00499819995373445\n",
            "train loss:0.007793637862354231\n",
            "train loss:0.00836075856624929\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.961\n",
            "Saved Network Parameters!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.6 CNN 시각화하기"
      ],
      "metadata": {
        "id": "ysvnUHxW900n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.6.1 1번째 층의 가중치 시각화하기"
      ],
      "metadata": {
        "id": "qcnE7qE893zg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***ch07/visualize.filter.py***"
      ],
      "metadata": {
        "id": "2Qh8sw2997Tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#from simple_convnet import SimpleConvNet\n",
        "\n",
        "def filter_show(filters, nx=8, margin=3, scale=10):\n",
        "    \"\"\"\n",
        "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
        "    \"\"\"\n",
        "    FN, C, FH, FW = filters.shape\n",
        "    ny = int(np.ceil(FN / nx))\n",
        "\n",
        "    fig = plt.figure()\n",
        "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "    for i in range(FN):\n",
        "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
        "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "U1Pps8Ww-KdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = SimpleConvNet()\n",
        "# (학습 전) 무작위 초기화 후의 가중치\n",
        "filter_show(network.params['W1'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "jw-zVaUR-vBH",
        "outputId": "b43b2600-7717-46fe-bd9a-6b76588aa448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 30 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 후\n",
        "network.load_params(\"params.pkl\")\n",
        "filter_show(network.params['W1'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "EfAvU-VS-rKn",
        "outputId": "51b5ac47-4ab6-4bcd-dc41-735116d90944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 30 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}